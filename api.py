import asyncio
import logging
import random
import re
import os
import time
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from playwright.async_api import async_playwright, TimeoutError
from typing import List, Optional, Dict, Any
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from fake_useragent import UserAgent, FakeUserAgentError
from stealth_config import (
    get_stealth_browser_args, 
    get_random_stealth_config, 
    get_stealth_page_scripts,
    get_realistic_headers,
    get_human_like_delay
)
from proxy_manager import get_smart_proxy_config, record_proxy_result, proxy_manager

# --- é…ç½® ---
PROXY_FILE = "proxies.txt"
# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–API_TOKENï¼Œå¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªé»˜è®¤çš„ã€ä»…ä¾›æœ¬åœ°æµ‹è¯•çš„ä»¤ç‰Œ
API_TOKEN = os.getenv("API_TOKEN", "local-dev-token")

# äº‘ä»£ç†é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
CLOUD_PROXY_USERNAME = os.getenv("CLOUD_PROXY_USERNAME")
CLOUD_PROXY_PASSWORD = os.getenv("CLOUD_PROXY_PASSWORD") 
CLOUD_PROXY_HOST = os.getenv("CLOUD_PROXY_HOST")
CLOUD_PROXY_PORT = os.getenv("CLOUD_PROXY_PORT", "1080")

# ä»£ç†è½®æ¢ç­–ç•¥
PROXY_ROTATION_STRATEGY = os.getenv("PROXY_ROTATION_STRATEGY", "random")  # random, sequential, session
DISABLE_PROXY = os.getenv("DISABLE_PROXY", "false").lower() == "true"  # æ˜¯å¦ç¦ç”¨ä»£ç†

# GCP Cloud Function/Cloud Run ç‰¹å®šé…ç½®
IS_CLOUD_FUNCTION = os.getenv("FUNCTION_TARGET") is not None
IS_CLOUD_RUN = os.getenv("K_SERVICE") is not None  # Cloud Run ç¯å¢ƒå˜é‡
GCP_REGION = os.getenv("FUNCTION_REGION", "us-central1")
USE_GCP_NATURAL_IP_ROTATION = os.getenv("USE_GCP_NATURAL_IP_ROTATION", "true").lower() == "true"

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# åˆ›å»ºè‡ªå®šä¹‰æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger("ifood_api")
logger.setLevel(logging.INFO)

# æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# åœ¨ Cloud Function ç¯å¢ƒä¸­æ·»åŠ æ–‡ä»¶æ—¥å¿—
if IS_CLOUD_FUNCTION:
    try:
        file_handler = logging.FileHandler("/tmp/ifood_api.log")
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        logger.info("å·²å¯ç”¨æ–‡ä»¶æ—¥å¿—è®°å½•")
    except Exception as e:
        logger.warning(f"æ— æ³•åˆ›å»ºæ–‡ä»¶æ—¥å¿—å¤„ç†å™¨: {e}")

# --- User-Agenté…ç½® ---
try:
    ua = UserAgent(fallback="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
except FakeUserAgentError as e:
    # If there's an issue fetching user agents (e.g., network error, outdated db),
    # fall back to a generic but modern-looking User-Agent.
    logging.warning(f"fake-useragent åˆå§‹åŒ–å¤±è´¥: {e}. å°†ä½¿ç”¨å¤‡ç”¨ User-Agent.")
    ua = None

def get_random_user_agent():
    """è·å–ä¸€ä¸ªéšæœºçš„User-Agentï¼Œå¦‚æœåº“åˆå§‹åŒ–å¤±è´¥åˆ™è¿”å›ä¸€ä¸ªå¤‡ç”¨å€¼ã€‚"""
    if ua:
        return ua.random
    return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"

# --- å®‰å…¨è®¤è¯ ---
security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """ä¾èµ–é¡¹ï¼Œç”¨äºéªŒè¯Bearer Tokenã€‚"""
    if credentials.scheme != "Bearer" or credentials.credentials != API_TOKEN:
        raise HTTPException(
            status_code=401, 
            detail="æ— æ•ˆçš„è®¤è¯ä»¤ç‰Œ",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return credentials.credentials

# --- FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(
    title="iFood Menu API",
    description="ä¸€ä¸ªé€šè¿‡æ‹¦æˆªç½‘ç»œè¯·æ±‚æ¥è·å–iFoodåº—é“ºèœå•çš„APIæœåŠ¡ã€‚",
    version="1.0.0",
)

# --- æ·»åŠ CORSä¸­é—´ä»¶ ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æºçš„è·¨åŸŸè¯·æ±‚
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰HTTPæ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)

# --- å…¨å±€å¼‚å¸¸å¤„ç†å™¨ ---
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨ï¼Œç»Ÿä¸€å¤„ç†æœªæ•è·çš„å¼‚å¸¸"""
    logger.error(f"æœªæ•è·çš„å¼‚å¸¸: {type(exc).__name__} - {str(exc)}", exc_info=True)
    
    return {
        "error": "InternalServerError",
        "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "detail": str(exc) if not IS_CLOUD_FUNCTION else "è¯·æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—",
        "timestamp": asyncio.get_event_loop().time()
    }

@app.exception_handler(TimeoutError)
async def timeout_exception_handler(request, exc):
    """è¶…æ—¶å¼‚å¸¸å¤„ç†å™¨"""
    logger.warning(f"è¯·æ±‚è¶…æ—¶: {str(exc)}")
    
    return {
        "error": "TimeoutError", 
        "message": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
        "detail": str(exc),
        "timestamp": asyncio.get_event_loop().time()
    }

# --- æ•°æ®æ¨¡å‹ ---
class StoreRequest(BaseModel):
    url: str

# --- ä»£ç†å¤„ç† ---
def get_cloud_proxy_config() -> Optional[Dict[str, str]]:
    """
    è·å–äº‘ä»£ç†é…ç½®ï¼Œæ”¯æŒè®¤è¯ã€‚
    ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„äº‘ä»£ç†æœåŠ¡ã€‚
    """
    if not all([CLOUD_PROXY_USERNAME, CLOUD_PROXY_PASSWORD, CLOUD_PROXY_HOST]):
        logging.warning("äº‘ä»£ç†é…ç½®ä¸å®Œæ•´ï¼Œå°†å°è¯•ä½¿ç”¨æœ¬åœ°ä»£ç†æ–‡ä»¶ã€‚")
        return None
    
    # æ„å»ºè®¤è¯URL
    auth_url = f"http://{CLOUD_PROXY_USERNAME}:{CLOUD_PROXY_PASSWORD}@{CLOUD_PROXY_HOST}:{CLOUD_PROXY_PORT}"
    
    proxy_config = {
        "server": auth_url
    }
    logging.info(f"ä½¿ç”¨äº‘ä»£ç†: {CLOUD_PROXY_HOST}:{CLOUD_PROXY_PORT}")
    return proxy_config

def get_local_proxy_config() -> Optional[Dict[str, str]]:
    """
    ä»æœ¬åœ°ä»£ç†æ–‡ä»¶ä¸­è¯»å–ä¸€ä¸ªéšæœºä»£ç†å¹¶è¿”å›Playwrightæ ¼å¼çš„é…ç½®å­—å…¸ã€‚
    æ”¯æŒçš„æ ¼å¼: host:port, # å¼€å¤´çš„è¡Œä¸ºæ³¨é‡Šã€‚
    å¦‚æœæ²¡æœ‰å¯ç”¨çš„ä»£ç†ï¼Œåˆ™è¿”å›Noneã€‚
    """
    try:
        with open(PROXY_FILE, 'r') as f:
            proxies = [
                line.strip() for line in f 
                if line.strip() and not line.strip().startswith('#') and ':' in line
            ]
        if not proxies:
            logging.warning("ä»£ç†æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ä»£ç†ã€‚")
            return None
        
        chosen_proxy = random.choice(proxies)
        host, port = chosen_proxy.split(':', 1)
        
        proxy_config = {
            "server": f"socks5://{host}:{port}"
        }
        logging.info(f"å·²é€‰æ‹©æœ¬åœ°ä»£ç†: {proxy_config['server']}")
        return proxy_config
    except FileNotFoundError:
        logging.warning(f"è­¦å‘Š: ä»£ç†æ–‡ä»¶ '{PROXY_FILE}' æœªæ‰¾åˆ°ã€‚")
        return None
    except Exception as e:
        logging.error(f"è§£ææœ¬åœ°ä»£ç†æ—¶å‡ºé”™: {e}")
        return None

def get_gcp_natural_ip_config() -> Optional[Dict[str, str]]:
    """
    åœ¨Cloud Functionç¯å¢ƒä¸­ï¼Œåˆ©ç”¨GCPçš„è‡ªç„¶IPè½®æ¢ç‰¹æ€§ã€‚
    æ¯æ¬¡å‡½æ•°è°ƒç”¨éƒ½å¯èƒ½ä½¿ç”¨ä¸åŒçš„å‡ºå£IPã€‚
    """
    if not IS_CLOUD_FUNCTION:
        return None
    
    if not USE_GCP_NATURAL_IP_ROTATION:
        return None
    
    logging.info(f"ä½¿ç”¨GCPè‡ªç„¶IPè½®æ¢ (åŒºåŸŸ: {GCP_REGION})")
    # åœ¨Cloud Functionä¸­ï¼Œä¸è®¾ç½®ä»£ç†é…ç½®ï¼Œè®©GCPè‡ªåŠ¨åˆ†é…IP
    return None

def get_random_proxy_config() -> Optional[Dict[str, str]]:
    """
    å¢å¼ºçš„æ™ºèƒ½ä»£ç†é€‰æ‹©ç­–ç•¥ï¼š
    1. å¦‚æœç¦ç”¨ä»£ç†ï¼Œç›´æ¥è¿”å›None
    2. åœ¨Cloud Functionç¯å¢ƒä¸­ï¼Œä¼˜å…ˆä½¿ç”¨GCPè‡ªç„¶IPè½®æ¢
    3. ä½¿ç”¨æ™ºèƒ½ä»£ç†ç®¡ç†å™¨é€‰æ‹©æœ€ä½³ä»£ç†
    4. å¦‚æœé…ç½®äº†äº‘ä»£ç†ï¼Œä½¿ç”¨äº‘ä»£ç†
    5. æœ€åç›´æ¥è¿æ¥
    
    è¿”å›Playwrightæ ¼å¼çš„ä»£ç†é…ç½®å­—å…¸ã€‚
    """
    # 0. æ£€æŸ¥æ˜¯å¦ç¦ç”¨ä»£ç†
    if DISABLE_PROXY:
        logging.info("ä»£ç†åŠŸèƒ½å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨GCPè‡ªç„¶IPè½®æ¢")
        return None
    
    # 1. åœ¨Cloud Functionç¯å¢ƒä¸­ï¼Œå¼ºåˆ¶ä½¿ç”¨GCPè‡ªç„¶IPè½®æ¢ï¼Œä¸ä½¿ç”¨ä»»ä½•ä»£ç†
    if IS_CLOUD_FUNCTION:
        logging.info("åœ¨Cloud Functionç¯å¢ƒä¸­ï¼Œå¼ºåˆ¶ä½¿ç”¨GCPè‡ªç„¶IPè½®æ¢ï¼Œä¸ä½¿ç”¨ä»£ç†")
        return None  # è¿”å›Noneè¡¨ç¤ºä¸ä½¿ç”¨ä»£ç†ï¼Œè®©GCPè‡ªåŠ¨åˆ†é…IP
    
    # 2. ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½ä»£ç†ç®¡ç†å™¨ï¼ˆä»…åœ¨éCloud Functionç¯å¢ƒä¸­ï¼‰
    smart_proxy = get_smart_proxy_config()
    if smart_proxy:
        logging.info(f"ä½¿ç”¨æ™ºèƒ½ä»£ç†ç®¡ç†å™¨é€‰æ‹©çš„ä»£ç†: {smart_proxy.get('server', 'unknown')}")
        return smart_proxy
    
    # 3. å›é€€åˆ°äº‘ä»£ç†ï¼ˆä»…åœ¨éCloud Functionç¯å¢ƒä¸­ï¼‰
    cloud_proxy = get_cloud_proxy_config()
    if cloud_proxy:
        logging.info("ä½¿ç”¨äº‘ä»£ç†é…ç½®")
        return cloud_proxy
    
    # 4. æœ€ç»ˆå›é€€
    logging.info("æœªæ‰¾åˆ°å¯ç”¨çš„ä»£ç†é…ç½®ï¼Œå°†ç›´æ¥è¿æ¥ã€‚")
    return None

def clean_url(url: str) -> str:
    """
    æ¸…ç†URLï¼Œå»æ‰é—®å·åé¢çš„æŸ¥è¯¢å‚æ•°éƒ¨åˆ†ã€‚
    
    Args:
        url: åŸå§‹URL
        
    Returns:
        æ¸…ç†åçš„URLï¼Œä¸åŒ…å«æŸ¥è¯¢å‚æ•°
    """
    if '?' in url:
        cleaned_url = url.split('?')[0]
        logging.info(f"URLå·²æ¸…ç†: {url} -> {cleaned_url}")
        return cleaned_url
    return url

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ (é‡æ„å) ---

def _get_optimized_browser_args() -> List[str]:
    """
    è·å–å¢å¼ºçš„åæ£€æµ‹æµè§ˆå™¨å¯åŠ¨å‚æ•°
    ç»“åˆäº†Browserlessæœ€ä½³å®è·µå’Œé«˜çº§åæ£€æµ‹æŠ€æœ¯
    """
    # ä½¿ç”¨æ–°çš„éšèº«é…ç½®
    base_args = get_stealth_browser_args()
    
    # åœ¨ Cloud Function ç¯å¢ƒä¸­è¿›ä¸€æ­¥ä¼˜åŒ–
    if IS_CLOUD_FUNCTION or IS_CLOUD_RUN:
        base_args.extend([
            "--single-process",  # å‡å°‘å†…å­˜å ç”¨
            "--disable-images",  # ç¦ç”¨å›¾ç‰‡åŠ è½½
            "--disable-site-isolation-trials",
            "--disable-speech-api",
            "--metrics-recording-only",
            "--safebrowsing-disable-auto-update",
        ])
        
        # è®¾ç½®é€‚åˆäº‘ç¯å¢ƒçš„çª—å£å¤§å°
        base_args.append("--window-size=1366,768")
    
    return base_args

async def _launch_browser_with_fallback(playwright_instance, launch_options: Dict[str, Any]):
    """
    ä½¿ç”¨åå¤‡ç­–ç•¥å¯åŠ¨æµè§ˆå™¨ï¼Œå‚è€ƒ Browserless å»ºè®®çš„é”™è¯¯å¤„ç†æ–¹å¼ã€‚
    """
    strategies = [
        {
            "name": "æ ‡å‡†å¯åŠ¨",
            "options": launch_options
        },
        {
            "name": "æœ€å°åŒ–å¯åŠ¨", 
            "options": {
                **launch_options,
                "args": [
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--single-process"
                ]
            }
        },
        {
            "name": "æ— å¤´æµè§ˆå™¨å¯åŠ¨",
            "options": {
                **launch_options,
                "args": [
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--single-process",
                    "--disable-extensions",
                    "--disable-plugins",
                    "--disable-images",
                    "--disable-javascript",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor"
                ]
            }
        }
    ]
    
    if IS_CLOUD_FUNCTION:
        # Cloud Function ç¯å¢ƒçš„ç‰¹æ®Šç­–ç•¥
        strategies.append({
            "name": "Cloud Function æµè§ˆå™¨å¯åŠ¨",
            "options": {
                **launch_options,
                "executable_path": "/www-data-home/.cache/ms-playwright/chromium-*/chrome-linux/chrome",
                "args": [
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage", 
                    "--disable-gpu",
                    "--single-process"
                ]
            }
        })
        
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿ Chrome (å¦‚æœå­˜åœ¨)
        strategies.append({
            "name": "ç³»ç»Ÿ Chrome å¯åŠ¨",
            "options": {
                **launch_options,
                "executable_path": "/usr/bin/google-chrome-stable",
                "args": [
                    "--no-sandbox",
                    "--disable-setuid-sandbox", 
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--headless"
                ]
            }
        })
        
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿ Chromium (å¦‚æœå­˜åœ¨)
        strategies.append({
            "name": "ç³»ç»Ÿ Chromium å¯åŠ¨",
            "options": {
                **launch_options,
                "executable_path": "/usr/bin/chromium-browser",
                "args": [
                    "--no-sandbox",
                    "--disable-setuid-sandbox", 
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--headless"
                ]
            }
        })
        
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿ Chrome (å¤‡ç”¨è·¯å¾„)
        strategies.append({
            "name": "ç³»ç»Ÿ Chrome å¤‡ç”¨å¯åŠ¨",
            "options": {
                **launch_options,
                "executable_path": "/usr/bin/chromium",
                "args": [
                    "--no-sandbox",
                    "--disable-setuid-sandbox", 
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--headless"
                ]
            }
        })
        
        strategies.append({
            "name": "ç³»ç»Ÿæµè§ˆå™¨æŸ¥æ‰¾å¯åŠ¨",
            "options": None  # ç‰¹æ®Šæ ‡è®°
        })
        
        strategies.append({
            "name": "Cloud Function ç¯å¢ƒå®‰è£…",
            "options": None  # ç‰¹æ®Šæ ‡è®°
        })
        
        strategies.append({
            "name": "åŠ¨æ€å®‰è£…åå¯åŠ¨",
            "options": None  # ç‰¹æ®Šæ ‡è®°
        })
    
    last_error = None
    
    for strategy in strategies:
        try:
            logging.info(f"å°è¯•{strategy['name']}...")
            
            if strategy["name"] == "ç³»ç»Ÿæµè§ˆå™¨æŸ¥æ‰¾å¯åŠ¨":
                # åŠ¨æ€æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯ç”¨çš„æµè§ˆå™¨
                import subprocess
                import os
                logging.info("æ­£åœ¨æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯ç”¨çš„æµè§ˆå™¨...")
                
                # å¯èƒ½çš„æµè§ˆå™¨è·¯å¾„
                possible_browsers = [
                    "/usr/bin/google-chrome-stable",
                    "/usr/bin/chromium-browser", 
                    "/usr/bin/chromium",
                    "/usr/bin/google-chrome",
                    "/usr/bin/chrome",
                    "/snap/bin/chromium",
                    "/opt/google/chrome/chrome"
                ]
                
                found_browser = None
                for browser_path in possible_browsers:
                    if os.path.exists(browser_path):
                        found_browser = browser_path
                        logging.info(f"æ‰¾åˆ°ç³»ç»Ÿæµè§ˆå™¨: {found_browser}")
                        break
                
                if found_browser:
                    browser = await playwright_instance.chromium.launch(
                        headless=True,
                        timeout=30000,
                        executable_path=found_browser,
                        args=[
                            "--no-sandbox",
                            "--disable-setuid-sandbox",
                            "--disable-dev-shm-usage", 
                            "--disable-gpu",
                            "--single-process"
                        ]
                    )
                else:
                    # å°è¯•ä½¿ç”¨ which å‘½ä»¤æŸ¥æ‰¾
                    try:
                        result = subprocess.run(
                            ["which", "chromium-browser"], 
                            capture_output=True, 
                            text=True,
                            timeout=10
                        )
                        if result.returncode == 0:
                            found_browser = result.stdout.strip()
                            logging.info(f"é€šè¿‡ which æ‰¾åˆ°æµè§ˆå™¨: {found_browser}")
                            
                            browser = await playwright_instance.chromium.launch(
                                headless=True,
                                timeout=30000,
                                executable_path=found_browser,
                                args=[
                                    "--no-sandbox",
                                    "--disable-setuid-sandbox",
                                    "--disable-dev-shm-usage", 
                                    "--disable-gpu",
                                    "--single-process"
                                ]
                            )
                        else:
                            raise Exception("æœªæ‰¾åˆ°ä»»ä½•ç³»ç»Ÿæµè§ˆå™¨")
                    except Exception as e:
                        logging.error(f"æŸ¥æ‰¾ç³»ç»Ÿæµè§ˆå™¨å¤±è´¥: {e}")
                        raise Exception("æœªæ‰¾åˆ°ä»»ä½•ç³»ç»Ÿæµè§ˆå™¨")
                        
            elif strategy["name"] == "Cloud Function ç¯å¢ƒå®‰è£…":
                # Cloud Function ç¯å¢ƒä¸­çš„æµè§ˆå™¨å®‰è£…
                import subprocess
                import sys
                import os
                logging.info("åœ¨ Cloud Function ç¯å¢ƒä¸­å®‰è£… Playwright æµè§ˆå™¨...")
                
                try:
                    # è®¾ç½®æµè§ˆå™¨è·¯å¾„ç¯å¢ƒå˜é‡
                    os.environ['PLAYWRIGHT_BROWSERS_PATH'] = '/www-data-home/.cache/ms-playwright'
                    
                    # åˆ›å»ºç›®å½•
                    os.makedirs('/www-data-home/.cache/ms-playwright', exist_ok=True)
                    
                    # å®‰è£…æµè§ˆå™¨
                    result = subprocess.run(
                        [sys.executable, "-m", "playwright", "install", "chromium"], 
                        capture_output=True, 
                        text=True,
                        timeout=300,
                        env={**os.environ, 'PLAYWRIGHT_BROWSERS_PATH': '/www-data-home/.cache/ms-playwright'}
                    )
                    
                    if result.returncode != 0:
                        logging.warning(f"Cloud Function ç¯å¢ƒå®‰è£…å¤±è´¥: {result.stderr}")
                        raise Exception(f"å®‰è£…å¤±è´¥: {result.stderr}")
                    
                    logging.info("Cloud Function ç¯å¢ƒ Chromium å®‰è£…å®Œæˆ...")
                    
                    # æŸ¥æ‰¾å®‰è£…çš„æµè§ˆå™¨
                    import glob
                    browser_paths = glob.glob('/www-data-home/.cache/ms-playwright/chromium-*/chrome-linux/chrome')
                    
                    if browser_paths:
                        actual_browser_path = browser_paths[0]
                        logging.info(f"æ‰¾åˆ°æµè§ˆå™¨è·¯å¾„: {actual_browser_path}")
                        
                        browser = await playwright_instance.chromium.launch(
                            headless=True,
                            timeout=30000,
                            executable_path=actual_browser_path,
                            args=[
                                "--no-sandbox",
                                "--disable-setuid-sandbox",
                                "--disable-dev-shm-usage", 
                                "--disable-gpu",
                                "--single-process"
                            ]
                        )
                    else:
                        raise Exception("æœªæ‰¾åˆ°å®‰è£…çš„æµè§ˆå™¨")
                        
                except Exception as install_error:
                    logging.error(f"Cloud Function ç¯å¢ƒå®‰è£…å¤±è´¥: {install_error}")
                    raise install_error
                    
            elif strategy["name"] == "åŠ¨æ€å®‰è£…åå¯åŠ¨":
                # åŠ¨æ€å®‰è£…æµè§ˆå™¨ (å‚è€ƒ GitHub issue #1491)
                import subprocess
                import sys
                import os
                logging.info("æ­£åœ¨åŠ¨æ€å®‰è£… Playwright æµè§ˆå™¨...")
                
                try:
                    # è®¾ç½®æµè§ˆå™¨è·¯å¾„ç¯å¢ƒå˜é‡
                    os.environ['PLAYWRIGHT_BROWSERS_PATH'] = '/tmp/ms-playwright'
                    
                    # åˆ›å»ºä¸´æ—¶ç›®å½•
                    os.makedirs('/tmp/ms-playwright', exist_ok=True)
                    
                    # å®‰è£…æµè§ˆå™¨åˆ°ä¸´æ—¶ç›®å½•
                    result = subprocess.run(
                        [sys.executable, "-m", "playwright", "install", "chromium"], 
                        capture_output=True, 
                        text=True,
                        timeout=300,
                        env={**os.environ, 'PLAYWRIGHT_BROWSERS_PATH': '/tmp/ms-playwright'}
                    )
                    
                    if result.returncode != 0:
                        logging.warning(f"Playwright å®‰è£…å¤±è´¥: {result.stderr}")
                        raise Exception(f"å®‰è£…å¤±è´¥: {result.stderr}")
                    
                    logging.info("Chromium å®‰è£…å®Œæˆï¼Œé‡æ–°å¯åŠ¨...")
                    
                    # æŸ¥æ‰¾å®‰è£…çš„æµè§ˆå™¨
                    import glob
                    browser_paths = glob.glob('/tmp/ms-playwright/chromium-*/chrome-linux/chrome')
                    
                    if browser_paths:
                        actual_browser_path = browser_paths[0]
                        logging.info(f"æ‰¾åˆ°æµè§ˆå™¨è·¯å¾„: {actual_browser_path}")
                        
                        browser = await playwright_instance.chromium.launch(
                            headless=True,
                            timeout=30000,
                            executable_path=actual_browser_path,
                            args=[
                                "--no-sandbox",
                                "--disable-setuid-sandbox",
                                "--disable-dev-shm-usage", 
                                "--disable-gpu",
                                "--single-process"
                            ]
                        )
                    else:
                        raise Exception("æœªæ‰¾åˆ°å®‰è£…çš„æµè§ˆå™¨")
                        
                except Exception as install_error:
                    logging.error(f"åŠ¨æ€å®‰è£…å¤±è´¥: {install_error}")
                    raise install_error
            else:
                if strategy["name"] == "Cloud Function æµè§ˆå™¨å¯åŠ¨":
                    # å¤„ç†é€šé…ç¬¦è·¯å¾„
                    import glob
                    import os
                    
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„æµè§ˆå™¨è·¯å¾„
                    possible_paths = [
                        "/www-data-home/.cache/ms-playwright/chromium-*/chrome-linux/chrome",
                        "/root/.cache/ms-playwright/chromium-*/chrome-linux/chrome",
                        "/tmp/ms-playwright/chromium-*/chrome-linux/chrome",
                        "/app/.cache/ms-playwright/chromium-*/chrome-linux/chrome",
                        "/home/www-data/.cache/ms-playwright/chromium-*/chrome-linux/chrome"
                    ]
                    
                    browser_paths = []
                    for pattern in possible_paths:
                        browser_paths = glob.glob(pattern)
                        if browser_paths:
                            logging.info(f"åœ¨è·¯å¾„ {pattern} æ‰¾åˆ°æµè§ˆå™¨")
                            break
                    
                    if browser_paths:
                        actual_browser_path = browser_paths[0]
                        logging.info(f"æ‰¾åˆ°æµè§ˆå™¨è·¯å¾„: {actual_browser_path}")
                        
                        browser_options = strategy["options"].copy()
                        browser_options["executable_path"] = actual_browser_path
                        browser = await playwright_instance.chromium.launch(**browser_options)
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ°æµè§ˆå™¨ï¼Œå°è¯•åˆ—å‡ºç›®å½•å†…å®¹è¿›è¡Œè°ƒè¯•
                        logging.error("æœªæ‰¾åˆ°æµè§ˆå™¨ï¼Œå°è¯•åˆ—å‡ºå¯èƒ½çš„ç›®å½•:")
                        for path in ["/www-data-home/.cache", "/root/.cache", "/tmp", "/app/.cache", "/home/www-data/.cache"]:
                            try:
                                if os.path.exists(path):
                                    logging.error(f"ç›®å½• {path} å­˜åœ¨ï¼Œå†…å®¹: {os.listdir(path)}")
                                else:
                                    logging.error(f"ç›®å½• {path} ä¸å­˜åœ¨")
                            except Exception as e:
                                logging.error(f"æ— æ³•è®¿é—®ç›®å½• {path}: {e}")
                        
                        raise Exception(f"æœªæ‰¾åˆ°æµè§ˆå™¨ï¼Œå·²å°è¯•æ‰€æœ‰å¯èƒ½çš„è·¯å¾„")
                else:
                    browser = await playwright_instance.chromium.launch(**strategy["options"])
            
            logging.info(f"{strategy['name']}æˆåŠŸ")
            return browser
            
        except Exception as e:
            last_error = e
            logging.warning(f"{strategy['name']}å¤±è´¥: {str(e)}")
            continue
    
    # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥äº†
    raise Exception(f"æ— æ³•å¯åŠ¨æµè§ˆå™¨ï¼Œæœ€åé”™è¯¯: {str(last_error)}")

async def _process_api_response(key: str, response: Any) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªä» Playwright æ‹¦æˆªåˆ°çš„å“åº”ï¼Œå°†å…¶è½¬æ¢ä¸ºJSONæˆ–é”™è¯¯å­—å…¸ã€‚
    """
    if isinstance(response, Exception):
        return {"error": "ResponseError", "message": f"è·å– {key} ä¿¡æ¯å¤±è´¥: {type(response).__name__}"}
    if response is None:
        return {"error": "ResponseMissing", "message": f"æœªæ•è·åˆ° {key} APIå“åº”"}

    logging.info(f"æˆåŠŸæ‹¦æˆªåˆ° {key} API è¯·æ±‚: {response.url} [çŠ¶æ€: {response.status}]")
    if response.status == 403:
        return {"error": "Forbidden", "message": f"ä»£ç†IPåœ¨è·å– {key} æ—¶è¢«å°ç¦æˆ–æ‹’ç»è®¿é—®ã€‚", "status": 403}
    if response.ok:
        try:
            json_data = await response.json()
            
            # ä¸ºcatalog/menuæ•°æ®æ·»åŠ è¯¦ç»†æ—¥å¿—
            if key == "menu" and isinstance(json_data, dict):
                logging.info(f"=== CATALOG/MENU API å“åº”è¯¦æƒ… ===")
                logging.info(f"å“åº”URL: {response.url}")
                logging.info(f"å“åº”çŠ¶æ€: {response.status}")
                logging.info(f"å“åº”å¤´: {dict(response.headers)}")
                
                # è®°å½•catalogæ•°æ®ç»“æ„
                if "categories" in json_data:
                    categories_count = len(json_data["categories"])
                    logging.info(f"åˆ†ç±»æ•°é‡: {categories_count}")
                    
                    for i, category in enumerate(json_data["categories"]):
                        if isinstance(category, dict):
                            category_name = category.get("name", "æœªçŸ¥åˆ†ç±»")
                            items_count = len(category.get("items", []))
                            logging.info(f"  åˆ†ç±» {i+1}: {category_name} - å•†å“æ•°é‡: {items_count}")
                            
                            # è®°å½•å‰å‡ ä¸ªå•†å“çš„ä¿¡æ¯
                            items = category.get("items", [])
                            for j, item in enumerate(items[:3]):  # åªè®°å½•å‰3ä¸ªå•†å“
                                if isinstance(item, dict):
                                    item_name = item.get("name", "æœªçŸ¥å•†å“")
                                    item_price = item.get("price", "æ— ä»·æ ¼")
                                    logging.info(f"    å•†å“ {j+1}: {item_name} - ä»·æ ¼: {item_price}")
                            
                            if len(items) > 3:
                                logging.info(f"    ... è¿˜æœ‰ {len(items) - 3} ä¸ªå•†å“")
                        else:
                            logging.warning(f"  åˆ†ç±» {i+1} æ ¼å¼å¼‚å¸¸: {type(category)}")
                
                # è®°å½•å…¶ä»–é‡è¦å­—æ®µ
                for field in ["merchantId", "merchantName", "totalItems", "totalCategories"]:
                    if field in json_data:
                        logging.info(f"{field}: {json_data[field]}")
                
                logging.info(f"=== CATALOG/MENU API å“åº”è¯¦æƒ…ç»“æŸ ===")
            else:
                logging.info(f"{key} API å“åº”æ•°æ®: {json_data}")
            
            return json_data
        except Exception as json_error:
            logging.error(f"è§£æ {key} API JSONå“åº”æ—¶å‡ºé”™: {str(json_error)}")
            # å°è¯•è·å–åŸå§‹æ–‡æœ¬å†…å®¹
            try:
                raw_text = await response.text()
                logging.info(f"{key} API åŸå§‹å“åº”å†…å®¹ (å‰500å­—ç¬¦): {raw_text[:500]}")
                return {"error": "JSONParseError", "message": f"JSONè§£æå¤±è´¥: {str(json_error)}", "raw_content": raw_text[:1000]}
            except Exception as text_error:
                logging.error(f"è·å– {key} API åŸå§‹å“åº”å†…å®¹æ—¶å‡ºé”™: {str(text_error)}")
                return {"error": "ResponseParseError", "message": f"æ— æ³•è§£æå“åº”: {str(json_error)}"}
    else:
        return {"error": "APIError", "message": f"{key} APIè¿”å›é”™è¯¯çŠ¶æ€: {response.status}", "status": response.status}

async def _scrape_ifood_page(
    target_url: str,
    proxy_config: Optional[Dict[str, str]],
    api_patterns: Dict[str, re.Pattern]
) -> Dict[str, Any]:
    """
    ä¸€ä¸ªé€šç”¨çš„æŠ“å–å‡½æ•°ï¼Œå¤„ç†æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸã€æ‹¦æˆªAPIè¯·æ±‚å¹¶è¿”å›å¤„ç†åçš„JSONæ•°æ®ã€‚
    é’ˆå¯¹Cloud Functionç¯å¢ƒè¿›è¡Œäº†ä¼˜åŒ–ï¼Œå‚è€ƒ Browserless æœ€ä½³å®è·µã€‚
    """
    async with async_playwright() as p:
        browser = None
        try:
            # æ ¹æ®ç¯å¢ƒå˜é‡è·å–è¶…æ—¶é…ç½®
            browser_timeout = int(os.getenv("BROWSER_TIMEOUT", "60")) * 1000  # é»˜è®¤60ç§’
            request_timeout = int(os.getenv("REQUEST_TIMEOUT", "90")) * 1000  # é»˜è®¤90ç§’
            
            # æ ¹æ® Browserless å»ºè®®ï¼Œä¸º Cloud Function ä¼˜åŒ–å¯åŠ¨é…ç½®
            launch_options = {
                "headless": True,
                "timeout": browser_timeout,  # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„å¯åŠ¨è¶…æ—¶
                "args": _get_optimized_browser_args()
            }
            
            # æ·»åŠ ä»£ç†é…ç½®
            if proxy_config:
                launch_options["proxy"] = proxy_config
                logging.info("æ­£åœ¨é€šè¿‡ä»£ç†å¯åŠ¨æµè§ˆå™¨...")
            else:
                if IS_CLOUD_FUNCTION:
                    logging.info("åœ¨Cloud Functionç¯å¢ƒä¸­ï¼Œä½¿ç”¨GCPåŠ¨æ€IPå¯åŠ¨æµè§ˆå™¨...")
                else:
                    logging.info("æœªæä¾›ä»£ç†ï¼Œæ­£åœ¨ç›´æ¥å¯åŠ¨æµè§ˆå™¨...")
            
            # ç»Ÿä¸€çš„æµè§ˆå™¨å¯åŠ¨é€»è¾‘
            browser = await _launch_browser_with_fallback(p, launch_options)
            
            # è·å–éšæœºéšèº«é…ç½®
            stealth_config = get_random_stealth_config()
            
            # åˆ›å»ºé¡µé¢å¹¶è®¾ç½®å¢å¼ºçš„åæ£€æµ‹é…ç½®
            logging.info(f"ä½¿ç”¨éšèº«é…ç½®: {stealth_config.user_agent[:50]}...")
            
            page = await browser.new_page(
                user_agent=stealth_config.user_agent,
                viewport={"width": stealth_config.viewport_width, "height": stealth_config.viewport_height},
                extra_http_headers=get_realistic_headers()
            )
            
            # è®¾ç½®åœ°ç†ä½ç½®æƒé™å’Œåæ ‡ï¼ˆåœ¨ä¸Šä¸‹æ–‡çº§åˆ«ï¼‰
            context = page.context
            await context.grant_permissions(['geolocation'])
            logging.info("è®¾ç½®åœ°ç†ä½ç½®ä¸ºåœ£ä¿ç½—...")
            await context.set_geolocation({"latitude": -23.5505, "longitude": -46.6333})  # åœ£ä¿ç½—åæ ‡
            
            # æ³¨å…¥åæ£€æµ‹è„šæœ¬
            for script in get_stealth_page_scripts():
                await page.add_init_script(script)
            
            # æ³¨å…¥åœ°ç†ä½ç½®æ¨¡æ‹Ÿè„šæœ¬
            geolocation_script = """
                // è¦†ç›–åœ°ç†ä½ç½®APIï¼Œå¼ºåˆ¶è¿”å›åœ£ä¿ç½—åæ ‡
                Object.defineProperty(navigator.geolocation, 'getCurrentPosition', {
                    value: function(success, error, options) {
                        const position = {
                            coords: {
                                latitude: -23.5505,
                                longitude: -46.6333,
                                accuracy: 10,
                                altitude: null,
                                altitudeAccuracy: null,
                                heading: null,
                                speed: null
                            },
                            timestamp: Date.now()
                        };
                        if (success) {
                            success(position);
                        }
                    }
                });
                
                Object.defineProperty(navigator.geolocation, 'watchPosition', {
                    value: function(success, error, options) {
                        const position = {
                            coords: {
                                latitude: -23.5505,
                                longitude: -46.6333,
                                accuracy: 10,
                                altitude: null,
                                altitudeAccuracy: null,
                                heading: null,
                                speed: null
                            },
                            timestamp: Date.now()
                        };
                        if (success) {
                            success(position);
                        }
                        return 1; // è¿”å›watchId
                    }
                });
            """
            await page.add_init_script(geolocation_script)
            
            # è®¾ç½®é¡µé¢è¶…æ—¶å’Œé”™è¯¯å¤„ç†
            page.set_default_navigation_timeout(request_timeout)  # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„å¯¼èˆªè¶…æ—¶
            page.set_default_timeout(request_timeout)  # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„é»˜è®¤è¶…æ—¶
            
            # æ‹¦æˆªå¹¶ä¿®æ”¹APIè¯·æ±‚ï¼Œå¼ºåˆ¶æ·»åŠ åœ°ç†ä½ç½®å‚æ•°
            async def handle_api_request(route):
                request = route.request
                url = request.url
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦é˜»æ–­çš„èµ„æºï¼ˆå›¾ç‰‡ã€CSSç­‰ï¼‰
                if IS_CLOUD_FUNCTION and any(ext in url.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg', '.ico', '.css']):
                    await route.abort()
                    return
                
                # å¦‚æœæ˜¯iFoodçš„APIè¯·æ±‚ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯å¹¶ä¿®æ”¹
                if "cw-marketplace.ifood.com.br" in url or "merchant-info/graphql" in url:
                    # è®°å½•åŸå§‹è¯·æ±‚è¯¦æƒ…
                    logging.info(f"ğŸ“¡ æ‹¦æˆªåˆ°iFood APIè¯·æ±‚:")
                    logging.info(f"  URL: {url}")
                    logging.info(f"  æ–¹æ³•: {request.method}")
                    logging.info(f"  è¯·æ±‚å¤´: {dict(request.headers)}")
                    if request.post_data:
                        logging.info(f"  POSTæ•°æ®: {request.post_data}")
                    
                    # å‡†å¤‡ä¿®æ”¹åçš„URLå’Œè¯·æ±‚å¤´
                    modified_url = url
                    if "latitude=&" in url or "longitude=&" in url:
                        modified_url = url.replace("latitude=&longitude=", "latitude=-23.5505&longitude=-46.6333")
                        logging.info(f"ğŸ”§ ä¿®æ”¹APIè¯·æ±‚URL: {url} -> {modified_url}")
                    
                    # æ·»åŠ å…³é”®çš„iFoodè¯·æ±‚å¤´
                    import uuid
                    import time
                    
                    additional_headers = {
                        'X-Ifood-Session-Id': str(uuid.uuid4()),
                        'X-Ifood-Device-Id': str(uuid.uuid4()),
                        'x-client-application-key': '41a266ee-51b7-4c37-9e9d-5cd331f280d5',
                        'platform': 'Desktop',
                        'app_version': '9.126.0',
                        'browser': 'Mac OS',
                        'x-device-model': 'Macintosh Chrome',
                        'Accept': 'application/json, text/plain, */*',
                        'Content-Type': 'application/json',
                        'Referer': 'https://www.ifood.com.br/',
                        'Origin': 'https://www.ifood.com.br'
                    }
                    
                    # æ£€æŸ¥å¹¶å¤„ç†cookies
                    try:
                        existing_cookie = request.headers.get('cookie', '')
                        if existing_cookie:
                            logging.info(f"ğŸª ç°æœ‰Cookieå¤´: {existing_cookie[:200]}...")
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«PXç›¸å…³cookies
                            if '_px' in existing_cookie or 'pxcts' in existing_cookie:
                                logging.info("âœ… æ£€æµ‹åˆ°PXç›¸å…³cookiesåœ¨è¯·æ±‚ä¸­")
                            else:
                                logging.warning("âš ï¸ è¯·æ±‚ä¸­ç¼ºå°‘PXç›¸å…³cookies")
                        else:
                            logging.warning("âš ï¸ è¯·æ±‚ä¸­å®Œå…¨ç¼ºå°‘Cookieå¤´ï¼Œå¯èƒ½å¯¼è‡´403é”™è¯¯")
                            
                            # å°è¯•ä»é¡µé¢ä¸Šä¸‹æ–‡è·å–cookieså¹¶æ·»åŠ åˆ°è¯·æ±‚ä¸­
                            # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªå°è¯•æ€§çš„ä¿®å¤ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´
                            logging.info("å°è¯•ä»é¡µé¢ä¸Šä¸‹æ–‡è·å–cookies...")
                    except Exception as e:
                        logging.warning(f"æ£€æŸ¥Cookieæ—¶å‡ºé”™: {e}")
                    
                    logging.info(f"ğŸ”§ æ·»åŠ å…³é”®è¯·æ±‚å¤´: {additional_headers}")
                    
                    # ç»§ç»­è¯·æ±‚å¹¶æ·»åŠ è¯·æ±‚å¤´
                    await route.continue_(url=modified_url, headers={**dict(request.headers), **additional_headers})
                else:
                    # æ­£å¸¸ç»§ç»­è¯·æ±‚
                    await route.continue_()
            
            # åº”ç”¨ç»Ÿä¸€çš„è¯·æ±‚æ‹¦æˆªå™¨
            await page.route("**/*", handle_api_request)
            
            # è®¾ç½®æ›´å®½æ¾çš„ç½‘ç»œç­–ç•¥ï¼Œå‡å°‘è¶…æ—¶ï¼Œå¹¶æ·»åŠ iFoodç‰¹å®šçš„è¯·æ±‚å¤´
            await page.set_extra_http_headers({
                'Accept-Language': 'pt-BR,pt;q=1',
                'Accept-Encoding': 'gzip, deflate, br, zstd',
                'Cache-Control': 'no-cache, no-store',
                'Pragma': 'no-cache',
                'Accept': 'application/json, text/plain, */*',
                'Content-Type': 'application/json',
                'Referer': 'https://www.ifood.com.br/',
                'platform': 'Desktop',
                'app_version': '9.126.0',
                'browser': 'Mac OS',
                'x-device-model': 'Macintosh Chrome',
                'Origin': 'https://www.ifood.com.br',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site'
            })
            
            logging.info(f"æ­£åœ¨å¯¼èˆªåˆ°: {target_url}")

            # è®°å½•å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—å“åº”æ—¶é—´ï¼‰
            start_time = time.time()

            # å…ˆå¯¼èˆªåˆ°é¡µé¢ï¼Œç¡®ä¿å®Œå…¨æ¸²æŸ“
            logging.info("å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…å®Œå…¨æ¸²æŸ“...")
            await page.goto(target_url, wait_until='networkidle', timeout=request_timeout)
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ŒåŒ…æ‹¬æ‰€æœ‰JavaScript
            logging.info("ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½å’ŒJavaScriptæ‰§è¡Œ...")
            await page.wait_for_timeout(8000)  # å¢åŠ åˆ°8ç§’
            
            # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°ï¼Œç¡®ä¿é¡µé¢æ¸²æŸ“å®Œæˆ
            try:
                logging.info("ç­‰å¾…å…³é”®é¡µé¢å…ƒç´ å‡ºç°...")
                await page.wait_for_selector('body', timeout=10000)
                await page.wait_for_function('document.readyState === "complete"', timeout=10000)
                logging.info("é¡µé¢æ¸²æŸ“çŠ¶æ€æ£€æŸ¥å®Œæˆ")
            except Exception as e:
                logging.warning(f"ç­‰å¾…é¡µé¢å…ƒç´ æ—¶å‡ºé”™: {e}")
            
            # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
            logging.info("æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º...")
            try:
                # æ»šåŠ¨é¡µé¢
                await page.evaluate('window.scrollTo(0, 300)')
                await page.wait_for_timeout(1000)
                await page.evaluate('window.scrollTo(0, 0)')
                await page.wait_for_timeout(1000)
                
                # é¼ æ ‡ç§»åŠ¨å’Œç‚¹å‡»
                await page.mouse.move(200, 200)
                await page.wait_for_timeout(500)
                await page.mouse.move(400, 300)
                await page.wait_for_timeout(500)
                
                logging.info("ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿå®Œæˆ")
            except Exception as e:
                logging.warning(f"æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºæ—¶å‡ºé”™: {e}")
            
            # å†ç­‰å¾…ä¸€æ®µæ—¶é—´è®©åæœºå™¨äººç³»ç»Ÿå®Œå…¨åˆå§‹åŒ–
            logging.info("ç­‰å¾…åæœºå™¨äººç³»ç»Ÿå®Œå…¨åˆå§‹åŒ–...")
            await page.wait_for_timeout(5000)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰PXç›¸å…³çš„cookies
            cookies = await page.context.cookies()
            px_cookies = [c for c in cookies if 'px' in c['name'].lower()]
            all_cookies = [c for c in cookies if c['domain'] in ['.ifood.com.br', 'ifood.com.br', '.cw-marketplace.ifood.com.br']]
            
            if px_cookies:
                logging.info(f"æ£€æµ‹åˆ°PXåæœºå™¨äººcookies: {[c['name'] for c in px_cookies]}")
                for cookie in px_cookies:
                    logging.info(f"  PX Cookie: {cookie['name']} = {cookie['value'][:50]}...")
            else:
                logging.warning("æœªæ£€æµ‹åˆ°PXåæœºå™¨äººcookiesï¼Œå¯èƒ½ä¼šå¯¼è‡´403é”™è¯¯")
            
            if all_cookies:
                logging.info(f"æ£€æµ‹åˆ°æ‰€æœ‰ç›¸å…³cookies: {[c['name'] for c in all_cookies]}")
            else:
                logging.warning("æœªæ£€æµ‹åˆ°ä»»ä½•ç›¸å…³cookies")
                
            # å°è¯•æ‰‹åŠ¨è§¦å‘ä¸€äº›é¡µé¢äº¤äº’æ¥æ¿€æ´»PXç³»ç»Ÿ
            try:
                logging.info("å°è¯•è§¦å‘é¡µé¢äº¤äº’ä»¥æ¿€æ´»PXç³»ç»Ÿ...")
                await page.mouse.move(100, 100)
                await page.wait_for_timeout(1000)
                await page.mouse.move(200, 200)
                await page.wait_for_timeout(2000)
                
                # å†æ¬¡æ£€æŸ¥cookies
                cookies_after = await page.context.cookies()
                px_cookies_after = [c for c in cookies_after if 'px' in c['name'].lower()]
                if len(px_cookies_after) > len(px_cookies):
                    logging.info(f"é¡µé¢äº¤äº’åæ–°å¢PX cookies: {[c['name'] for c in px_cookies_after if c not in px_cookies]}")
            except Exception as e:
                logging.warning(f"é¡µé¢äº¤äº’æ—¶å‡ºé”™: {e}")
            
            # ç°åœ¨è®¾ç½®APIæ‹¦æˆªå¹¶é‡æ–°åŠ è½½é¡µé¢ä»¥è§¦å‘APIè°ƒç”¨
            logging.info(f"å¼€å§‹è®¾ç½®APIæ‹¦æˆªæ¨¡å¼ï¼Œç­‰å¾…ä»¥ä¸‹APIå“åº”:")
            for key, pattern in api_patterns.items():
                logging.info(f"  - {key}: {pattern.pattern}")
            
            response_awaitables = [
                page.wait_for_event(
                    "response",
                    lambda res, p=pattern: p.search(res.url),
                    timeout=request_timeout  # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„è¶…æ—¶æ—¶é—´
                )
                for pattern in api_patterns.values()
            ]
            
            # ä¸é‡æ–°åŠ è½½é¡µé¢ï¼Œè€Œæ˜¯é€šè¿‡JavaScriptè§¦å‘APIè°ƒç”¨
            logging.info("é€šè¿‡é¡µé¢äº¤äº’è§¦å‘APIè°ƒç”¨...")
            try:
                # å°è¯•ç‚¹å‡»é¡µé¢ä¸Šçš„å…ƒç´ æ¥è§¦å‘APIè°ƒç”¨
                await page.evaluate('''
                    // å°è¯•è§¦å‘é¡µé¢ä¸Šçš„äº‹ä»¶æ¥æ¿€æ´»APIè°ƒç”¨
                    window.dispatchEvent(new Event('focus'));
                    window.dispatchEvent(new Event('load'));
                    
                    // å¦‚æœé¡µé¢æœ‰ç‰¹å®šçš„è§¦å‘å™¨ï¼Œå°è¯•è°ƒç”¨
                    if (window.location.reload) {
                        setTimeout(() => {
                            window.location.reload();
                        }, 100);
                    }
                ''')
                
                # ç­‰å¾…APIè°ƒç”¨è¢«è§¦å‘
                navigation_awaitable = page.wait_for_load_state('networkidle', timeout=request_timeout)
                
            except Exception as e:
                logging.warning(f"JavaScriptè§¦å‘å¤±è´¥ï¼Œå›é€€åˆ°é¡µé¢é‡æ–°åŠ è½½: {e}")
                navigation_awaitable = page.reload(wait_until='networkidle', timeout=request_timeout)
            
            all_results = await asyncio.gather(
                *response_awaitables, navigation_awaitable, return_exceptions=True
            )
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = time.time() - start_time
            
            # ã€ä¿®å¤ã€‘å¢åŠ æ˜ç¡®çš„æ—¥å¿—è®°å½•ï¼Œä»¥è§£é‡Šä¸ºä½•å¤±è´¥
            # 1. æ£€æŸ¥å¯¼èˆªä»»åŠ¡æœ¬èº«æ˜¯å¦å¤±è´¥
            navigation_result = all_results[-1]
            if isinstance(navigation_result, Exception):
                logging.error(f"é¡µé¢å¯¼èˆªå¤±è´¥: {type(navigation_result).__name__} - {navigation_result}")
            else:
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æˆåŠŸæ‰“å¼€
                if hasattr(navigation_result, 'ok') and navigation_result.ok:
                    logging.info(f"âœ… é¡µé¢æˆåŠŸæ‰“å¼€: {target_url} [çŠ¶æ€: {navigation_result.status}]")
                elif hasattr(navigation_result, 'status'):
                    logging.warning(f"âš ï¸ é¡µé¢æ‰“å¼€ä½†è¿”å›å¼‚å¸¸çŠ¶æ€: {target_url} [çŠ¶æ€: {navigation_result.status}]")
                else:
                    logging.info(f"âœ… é¡µé¢å¯¼èˆªå®Œæˆ: {target_url}")

            # 2. æ£€æŸ¥å„ä¸ªAPIå“åº”çš„ç­‰å¾…ä»»åŠ¡æ˜¯å¦å¤±è´¥
            response_results = all_results[:-1]
            api_keys = list(api_patterns.keys())
            for i, res in enumerate(response_results):
                if isinstance(res, Exception):
                    logging.error(f"ç­‰å¾… API '{api_keys[i]}' å“åº”æ—¶å¤±è´¥: {type(res).__name__}")
            
            scraped_responses = dict(zip(api_keys, response_results))
            
            logging.info(f"APIæ‹¦æˆªç»“æœç»Ÿè®¡:")
            for key, response in scraped_responses.items():
                if isinstance(response, Exception):
                    logging.info(f"  - {key}: å¼‚å¸¸ - {type(response).__name__}")
                else:
                    logging.info(f"  - {key}: æˆåŠŸ - URL: {response.url}, çŠ¶æ€: {response.status}")

            # åœ¨æµè§ˆå™¨å…³é—­å‰å¤„ç†æ‰€æœ‰å“åº”
            processing_tasks = [
                _process_api_response(key, response)
                for key, response in scraped_responses.items()
            ]
            processed_results_list = await asyncio.gather(*processing_tasks)
            
            # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
            has_success = any("error" not in result for result in processed_results_list if isinstance(result, dict))
            
            # è®°å½•æœ€ç»ˆå¤„ç†ç»“æœ
            final_results = dict(zip(api_keys, processed_results_list))
            logging.info(f"æœ€ç»ˆå¤„ç†ç»“æœ:")
            for key, result in final_results.items():
                if isinstance(result, dict) and "error" in result:
                    logging.info(f"  - {key}: é”™è¯¯ - {result.get('error', 'Unknown')}: {result.get('message', 'No message')}")
                else:
                    if key == "menu" and isinstance(result, dict):
                        categories_count = len(result.get("categories", []))
                        logging.info(f"  - {key}: æˆåŠŸ - åŒ…å« {categories_count} ä¸ªåˆ†ç±»")
                    else:
                        logging.info(f"  - {key}: æˆåŠŸ - æ•°æ®ç±»å‹: {type(result)}")
            
            # è®°å½•ä»£ç†ä½¿ç”¨ç»“æœ
            if has_success:
                record_proxy_result(proxy_config, True, response_time)
            else:
                record_proxy_result(proxy_config, False, response_time, "all_apis_failed")
            
            return final_results

        except Exception as e:
            error_message = str(e)
            error_type = type(e).__name__
            
            # æ›´è¯¦ç»†çš„é”™è¯¯åˆ†ç±»å’Œå¤„ç†
            if isinstance(e, TimeoutError):
                error_message = "æ“ä½œè¶…æ—¶ã€‚å¯èƒ½åŸå› ï¼šé¡µé¢åŠ è½½è¿‡æ…¢ã€æœªè§¦å‘APIè¯·æ±‚ã€æˆ–è¢«CAPTCHAé˜»æŒ¡ã€‚"
                error_type = "TimeoutError"
            elif "net::ERR_" in error_message:
                error_message = "ç½‘ç»œè¿æ¥é”™è¯¯ã€‚å¯èƒ½æ˜¯ä»£ç†é—®é¢˜æˆ–ç½‘ç«™ä¸å¯è¾¾ã€‚"
                error_type = "NetworkError"
            elif "browserless" in error_message.lower():
                error_message = "æµè§ˆå™¨æœåŠ¡é”™è¯¯ã€‚å»ºè®®æ£€æŸ¥æµè§ˆå™¨é…ç½®æˆ–å†…å­˜é™åˆ¶ã€‚"
                error_type = "BrowserError"
            elif "memory" in error_message.lower() or "out of memory" in error_message.lower():
                error_message = "å†…å­˜ä¸è¶³é”™è¯¯ã€‚å»ºè®®ä¼˜åŒ–æµè§ˆå™¨å‚æ•°æˆ–å¢åŠ Cloud Functionå†…å­˜ã€‚"
                error_type = "MemoryError"
                
            logging.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ [{error_type}]: {error_message}")
            
            # è®°å½•ä»£ç†å¤±è´¥
            if 'start_time' in locals():
                response_time = time.time() - start_time
            else:
                response_time = 0.0
            record_proxy_result(proxy_config, False, response_time, error_type)
            
            # è®°å½•æ›´å¤šè°ƒè¯•ä¿¡æ¯ç”¨äº Cloud Function ç¯å¢ƒ
            if IS_CLOUD_FUNCTION:
                import psutil
                try:
                    memory_usage = psutil.virtual_memory()
                    logging.info(f"å½“å‰å†…å­˜ä½¿ç”¨: {memory_usage.percent}% ({memory_usage.used / 1024 / 1024:.1f}MB / {memory_usage.total / 1024 / 1024:.1f}MB)")
                except ImportError:
                    logging.info("psutil ä¸å¯ç”¨ï¼Œæ— æ³•è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯")
            
            return {
                key: {
                    "error": error_type, 
                    "message": error_message, 
                    "status": 500,
                    "is_cloud_function": IS_CLOUD_FUNCTION
                }
                for key in api_patterns.keys()
            }
        finally:
            # æ›´å¥½çš„èµ„æºæ¸…ç†ï¼Œå‚è€ƒ Browserless å»ºè®®
            if browser:
                try:
                    if browser.is_connected():
                        # å…ˆå…³é—­æ‰€æœ‰é¡µé¢
                        contexts = browser.contexts
                        for context in contexts:
                            await context.close()
                        
                        # ç„¶åå…³é—­æµè§ˆå™¨
                        await browser.close()
                        logging.info("æµè§ˆå™¨å’Œæ‰€æœ‰ä¸Šä¸‹æ–‡å·²å®‰å…¨å…³é—­ã€‚")
                    else:
                        logging.info("æµè§ˆå™¨å·²æ–­å¼€è¿æ¥ã€‚")
                except Exception as cleanup_error:
                    logging.warning(f"æ¸…ç†æµè§ˆå™¨èµ„æºæ—¶å‡ºé”™: {cleanup_error}")
                    
            # åœ¨ Cloud Function ç¯å¢ƒä¸­å¼ºåˆ¶åƒåœ¾å›æ”¶
            if IS_CLOUD_FUNCTION:
                import gc
                gc.collect()
                logging.info("å·²æ‰§è¡Œåƒåœ¾å›æ”¶ã€‚")

async def _scrape_ifood_page_dom_fallback(
    target_url: str,
    proxy_config: Optional[Dict[str, str]]
) -> Dict[str, Any]:
    """
    DOMè§£æå¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è§£æé¡µé¢å†…å®¹è€Œä¸ä¾èµ–APIæ‹¦æˆª
    """
    async with async_playwright() as p:
        browser = None
        try:
            # æ ¹æ®ç¯å¢ƒå˜é‡è·å–è¶…æ—¶é…ç½®
            browser_timeout = int(os.getenv("BROWSER_TIMEOUT", "90")) * 1000  # é»˜è®¤90ç§’
            request_timeout = int(os.getenv("REQUEST_TIMEOUT", "120")) * 1000  # é»˜è®¤120ç§’
            
            # å¯åŠ¨æµè§ˆå™¨
            launch_options = {
                "headless": True,
                "timeout": browser_timeout,
                "args": _get_optimized_browser_args()
            }
            
            if proxy_config:
                launch_options["proxy"] = proxy_config
                logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡ä»£ç†å¯åŠ¨æµè§ˆå™¨...")
            else:
                logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥å¯åŠ¨æµè§ˆå™¨...")
            
            browser = await _launch_browser_with_fallback(p, launch_options)
            
            # è·å–éšæœºéšèº«é…ç½®
            stealth_config = get_random_stealth_config()
            
            page = await browser.new_page(
                user_agent=stealth_config.user_agent,
                viewport={"width": stealth_config.viewport_width, "height": stealth_config.viewport_height},
                extra_http_headers=get_realistic_headers()
            )
            
            # è®¾ç½®åœ°ç†ä½ç½®æƒé™å’Œåæ ‡ï¼ˆDOMå¤‡ç”¨æ–¹æ¡ˆï¼‰
            context = page.context
            await context.grant_permissions(['geolocation'])
            logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šè®¾ç½®åœ°ç†ä½ç½®ä¸ºåœ£ä¿ç½—...")
            await context.set_geolocation({"latitude": -23.5505, "longitude": -46.6333})  # åœ£ä¿ç½—åæ ‡
            
            # æ³¨å…¥åæ£€æµ‹è„šæœ¬
            for script in get_stealth_page_scripts():
                await page.add_init_script(script)
            
            # æ³¨å…¥åœ°ç†ä½ç½®æ¨¡æ‹Ÿè„šæœ¬ï¼ˆDOMå¤‡ç”¨æ–¹æ¡ˆï¼‰
            geolocation_script = """
                // è¦†ç›–åœ°ç†ä½ç½®APIï¼Œå¼ºåˆ¶è¿”å›åœ£ä¿ç½—åæ ‡
                Object.defineProperty(navigator.geolocation, 'getCurrentPosition', {
                    value: function(success, error, options) {
                        const position = {
                            coords: {
                                latitude: -23.5505,
                                longitude: -46.6333,
                                accuracy: 10,
                                altitude: null,
                                altitudeAccuracy: null,
                                heading: null,
                                speed: null
                            },
                            timestamp: Date.now()
                        };
                        if (success) {
                            success(position);
                        }
                    }
                });
                
                Object.defineProperty(navigator.geolocation, 'watchPosition', {
                    value: function(success, error, options) {
                        const position = {
                            coords: {
                                latitude: -23.5505,
                                longitude: -46.6333,
                                accuracy: 10,
                                altitude: null,
                                altitudeAccuracy: null,
                                heading: null,
                                speed: null
                            },
                            timestamp: Date.now()
                        };
                        if (success) {
                            success(position);
                        }
                        return 1; // è¿”å›watchId
                    }
                });
            """
            await page.add_init_script(geolocation_script)
            
            # è®¾ç½®è¶…æ—¶
            page.set_default_navigation_timeout(request_timeout)
            page.set_default_timeout(request_timeout)
            
            # æ‹¦æˆªå¹¶ä¿®æ”¹APIè¯·æ±‚ï¼Œå¼ºåˆ¶æ·»åŠ åœ°ç†ä½ç½®å‚æ•°ï¼ˆDOMå¤‡ç”¨æ–¹æ¡ˆï¼‰
            async def handle_api_request_dom(route):
                request = route.request
                url = request.url
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦é˜»æ–­çš„èµ„æºï¼ˆå›¾ç‰‡ã€CSSç­‰ï¼‰
                if IS_CLOUD_FUNCTION and any(ext in url.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg', '.ico', '.css']):
                    await route.abort()
                    return
                
                # å¦‚æœæ˜¯iFoodçš„APIè¯·æ±‚ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯å¹¶ä¿®æ”¹ï¼ˆDOMå¤‡ç”¨æ–¹æ¡ˆï¼‰
                if "cw-marketplace.ifood.com.br" in url or "merchant-info/graphql" in url:
                    # è®°å½•åŸå§‹è¯·æ±‚è¯¦æƒ…
                    logging.info(f"ğŸ“¡ DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæ‹¦æˆªåˆ°iFood APIè¯·æ±‚:")
                    logging.info(f"  URL: {url}")
                    logging.info(f"  æ–¹æ³•: {request.method}")
                    logging.info(f"  è¯·æ±‚å¤´: {dict(request.headers)}")
                    if request.post_data:
                        logging.info(f"  POSTæ•°æ®: {request.post_data}")
                    
                    # å¦‚æœç¼ºå°‘åœ°ç†ä½ç½®å‚æ•°ï¼Œåˆ™ä¿®æ”¹URL
                    if "latitude=&" in url or "longitude=&" in url:
                        modified_url = url.replace("latitude=&longitude=", "latitude=-23.5505&longitude=-46.6333")
                        logging.info(f"ğŸ”§ DOMå¤‡ç”¨æ–¹æ¡ˆï¼šä¿®æ”¹APIè¯·æ±‚URL: {url} -> {modified_url}")
                        await route.continue_(url=modified_url)
                    else:
                        await route.continue_()
                else:
                    # æ­£å¸¸ç»§ç»­è¯·æ±‚
                    await route.continue_()
            
            # åº”ç”¨ç»Ÿä¸€çš„è¯·æ±‚æ‹¦æˆªå™¨ï¼ˆDOMå¤‡ç”¨æ–¹æ¡ˆï¼‰
            await page.route("**/*", handle_api_request_dom)
            
            logging.info(f"DOMå¤‡ç”¨æ–¹æ¡ˆï¼šå¯¼èˆªåˆ° {target_url}")
            
            # å¯¼èˆªåˆ°é¡µé¢ - åªç­‰å¾…å¯¼èˆªå¼€å§‹ï¼Œä¸ç­‰å¾…DOMåŠ è½½
            try:
                navigation_response = await page.goto(target_url, wait_until='commit', timeout=request_timeout)
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æˆåŠŸæ‰“å¼€
                if navigation_response and navigation_response.ok:
                    logging.info(f"âœ… DOMå¤‡ç”¨æ–¹æ¡ˆï¼šé¡µé¢æˆåŠŸæ‰“å¼€: {target_url} [çŠ¶æ€: {navigation_response.status}]")
                elif navigation_response:
                    logging.warning(f"âš ï¸ DOMå¤‡ç”¨æ–¹æ¡ˆï¼šé¡µé¢æ‰“å¼€ä½†è¿”å›å¼‚å¸¸çŠ¶æ€: {target_url} [çŠ¶æ€: {navigation_response.status}]")
                else:
                    logging.warning(f"âš ï¸ DOMå¤‡ç”¨æ–¹æ¡ˆï¼šé¡µé¢å¯¼èˆªå®Œæˆä½†æ— å“åº”ä¿¡æ¯: {target_url}")
            except Exception as nav_error:
                logging.error(f"âŒ DOMå¤‡ç”¨æ–¹æ¡ˆï¼šé¡µé¢å¯¼èˆªå¤±è´¥: {target_url} - {type(nav_error).__name__}: {nav_error}")
                raise nav_error
            
            # ä¸ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œç›´æ¥è¿›è¡ŒAPIæ‹¦æˆª
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
            await page.wait_for_timeout(3000)
            
            # æ£€æŸ¥é¡µé¢å†…å®¹ï¼Œçœ‹æ˜¯å¦éœ€è¦åœ°å€è¾“å…¥
            try:
                page_content = await page.content()
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«åœ°å€ç›¸å…³çš„æ–‡æœ¬
                address_indicators = [
                    "Escolha um endereÃ§o", "Informe seu endereÃ§o", 
                    "VocÃª verÃ¡ apenas os restaurantes", "entregam onde vocÃª estÃ¡"
                ]
                
                needs_address = any(indicator in page_content for indicator in address_indicators)
                
                if needs_address:
                    logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæ£€æµ‹åˆ°éœ€è¦åœ°å€çš„é¡µé¢å†…å®¹ï¼Œå°è¯•å¤„ç†åœ°å€è¾“å…¥...")
                    
                    # å°è¯•æŸ¥æ‰¾å¹¶ç‚¹å‡»åœ°å€ç›¸å…³çš„æŒ‰é’®æˆ–è¾“å…¥æ¡†
                    address_triggers = [
                        'button:has-text("Informe seu endereÃ§o")',
                        'button:has-text("Escolha um endereÃ§o")',
                        'button:has-text("Ignorar")',
                        'button:has-text("Informar")',
                        'div:has-text("Informe seu endereÃ§o")',
                        'div:has-text("Escolha um endereÃ§o")',
                        'input[placeholder*="endereÃ§o"]',
                        'input[placeholder*="CEP"]',
                        'input[placeholder*="address"]',
                        'input[type="text"]',
                        '[data-testid="address-input"]',
                        '.address-input',
                        '[data-testid="location-input"]',
                        '.location-input'
                    ]
                    
                    address_triggered = False
                    address_attempts = 0
                    
                    # é¦–å…ˆå°è¯•ç‚¹å‡»"Ignorar"æŒ‰é’®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    try:
                        ignore_button = await page.query_selector('button:has-text("Ignorar")')
                        if ignore_button:
                            await ignore_button.click()
                            await page.wait_for_timeout(2000)
                            logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šå·²ç‚¹å‡»'Ignorar'æŒ‰é’®ï¼Œå°è¯•è·³è¿‡åœ°å€è¾“å…¥")
                            
                            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·³è¿‡
                            new_content = await page.content()
                            if not any(indicator in new_content for indicator in address_indicators):
                                address_triggered = True
                                logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæˆåŠŸè·³è¿‡åœ°å€è¾“å…¥")
                    except Exception as e:
                        logging.info(f"å°è¯•ç‚¹å‡»'Ignorar'æŒ‰é’®æ—¶å‡ºé”™: {str(e)}")
                    
                    # å¦‚æœè·³è¿‡å¤±è´¥ï¼Œå°è¯•è¾“å…¥åœ°å€
                    if not address_triggered:
                        for selector in address_triggers:
                            if address_attempts >= 3:  # é™åˆ¶å°è¯•æ¬¡æ•°
                                break
                                
                            try:
                                elements = await page.query_selector_all(selector)
                                for element in elements:
                                    try:
                                        # å¦‚æœæ˜¯è¾“å…¥æ¡†ï¼Œç›´æ¥è¾“å…¥
                                        tag_name = await element.evaluate('el => el.tagName.toLowerCase()')
                                        if tag_name == 'input':
                                            # å°è¯•å¤šä¸ªåœ°å€æ ¼å¼
                                            test_addresses = [
                                                "Rua Augusta, 123, SÃ£o Paulo, SP",
                                                "Av. Paulista, 1000, SÃ£o Paulo, SP",
                                                "Rua Oscar Freire, 456, SÃ£o Paulo, SP"
                                            ]
                                            
                                            for address in test_addresses:
                                                try:
                                                    await element.fill(address)
                                                    await page.wait_for_timeout(1000)
                                                    await element.press('Enter')
                                                    await page.wait_for_timeout(3000)
                                                    
                                                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                                                    current_content = await page.content()
                                                    if not any(indicator in current_content for indicator in address_indicators):
                                                        logging.info(f"DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæˆåŠŸè¾“å…¥åœ°å€: {address}")
                                                        address_triggered = True
                                                        break
                                                except Exception:
                                                    continue
                                            
                                            if address_triggered:
                                                break
                                        else:
                                            # å¦‚æœæ˜¯æŒ‰é’®æˆ–divï¼Œç‚¹å‡»å®ƒ
                                            await element.click()
                                            await page.wait_for_timeout(2000)
                                            logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šå·²ç‚¹å‡»åœ°å€ç›¸å…³å…ƒç´ ")
                                            
                                            # ç‚¹å‡»åå¯èƒ½å‡ºç°è¾“å…¥æ¡†ï¼Œå†æ¬¡å°è¯•è¾“å…¥
                                            new_inputs = await page.query_selector_all('input[type="text"], input[placeholder*="endereÃ§o"], input[placeholder*="CEP"]')
                                            for input_elem in new_inputs:
                                                try:
                                                    await input_elem.fill("Rua Augusta, 123, SÃ£o Paulo, SP")
                                                    await page.wait_for_timeout(1000)
                                                    await input_elem.press('Enter')
                                                    await page.wait_for_timeout(3000)
                                                    logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šå·²åœ¨å¼¹å‡ºçš„è¾“å…¥æ¡†ä¸­è¾“å…¥åœ°å€")
                                                    address_triggered = True
                                                    break
                                                except Exception:
                                                    continue
                                            
                                            if address_triggered:
                                                break
                                    except Exception as e:
                                        logging.warning(f"å¤„ç†åœ°å€å…ƒç´ æ—¶å‡ºé”™: {str(e)}")
                                        continue
                                
                                if address_triggered:
                                    break
                                    
                            except Exception as e:
                                logging.warning(f"æŸ¥æ‰¾åœ°å€å…ƒç´ æ—¶å‡ºé”™ [{selector}]: {str(e)}")
                                continue
                            
                            address_attempts += 1
                    
                    if address_triggered:
                        # ç­‰å¾…é¡µé¢æ›´æ–° - å¢åŠ ç­‰å¾…æ—¶é—´
                        logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šç­‰å¾…é¡µé¢æœç´¢å®Œæˆ...")
                        await page.wait_for_timeout(8000)  # å¢åŠ åˆ°8ç§’
                        
                        # ç­‰å¾…æœç´¢å®Œæˆ - æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜åœ¨æœç´¢çŠ¶æ€
                        search_complete = False
                        for attempt in range(15):  # æœ€å¤šç­‰å¾…30ç§’
                            try:
                                page_text = await page.text_content('body')
                                if page_text and 'Buscando por' not in page_text:
                                    # æ£€æŸ¥æ˜¯å¦å‡ºç°äº†èœå•å†…å®¹
                                    if 'menu' in page_text.lower() or 'cardÃ¡pio' in page_text.lower() or 'categoria' in page_text.lower():
                                        search_complete = True
                                        logging.info(f"DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæœç´¢å®Œæˆï¼Œæ£€æµ‹åˆ°èœå•å†…å®¹ (å°è¯• {attempt + 1})")
                                        break
                                    else:
                                        logging.info(f"DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæœç´¢å®Œæˆï¼Œä½†æœªæ£€æµ‹åˆ°èœå•å†…å®¹ (å°è¯• {attempt + 1})")
                                        # ç»§ç»­ç­‰å¾…ï¼Œå¯èƒ½èœå•è¿˜åœ¨åŠ è½½
                                        await page.wait_for_timeout(2000)
                                else:
                                    logging.info(f"DOMå¤‡ç”¨æ–¹æ¡ˆï¼šä»åœ¨æœç´¢ä¸­... (å°è¯• {attempt + 1})")
                                    await page.wait_for_timeout(2000)
                            except Exception:
                                await page.wait_for_timeout(2000)
                        
                        if not search_complete:
                            logging.warning("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæœç´¢å¯èƒ½æœªå®Œæˆï¼Œç»§ç»­å¤„ç†...")
                        
                        # ç»™é¡µé¢æ›´å¤šæ—¶é—´åŠ è½½èœå•å†…å®¹
                        await page.wait_for_timeout(5000)  # å¢åŠ åˆ°5ç§’
                        logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šåœ°å€è¾“å…¥å®Œæˆï¼Œç»§ç»­å¤„ç†")
                        
                        # è°ƒè¯•ï¼šä¿å­˜é¡µé¢æˆªå›¾ï¼ˆä»…åœ¨äº‘ç¯å¢ƒä¸­ï¼‰
                        if IS_CLOUD_FUNCTION or IS_CLOUD_RUN:
                            try:
                                await page.screenshot(path='/tmp/page_after_address.png')
                                logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šå·²ä¿å­˜åœ°å€è¾“å…¥åçš„é¡µé¢æˆªå›¾")
                            except Exception:
                                pass
                    else:
                        logging.warning("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæœªèƒ½æˆåŠŸè§¦å‘åœ°å€è¾“å…¥")
                        
                        # è°ƒè¯•ï¼šä¿å­˜å½“å‰é¡µé¢æˆªå›¾å’ŒHTML
                        if IS_CLOUD_FUNCTION or IS_CLOUD_RUN:
                            try:
                                await page.screenshot(path='/tmp/page_no_address.png')
                                with open('/tmp/page_content.html', 'w', encoding='utf-8') as f:
                                    f.write(page_content[:5000])  # åªä¿å­˜å‰5000å­—ç¬¦
                                logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šå·²ä¿å­˜æ— æ³•è¾“å…¥åœ°å€æ—¶çš„é¡µé¢æˆªå›¾å’Œå†…å®¹")
                            except Exception:
                                pass
                else:
                    logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šé¡µé¢ä¸éœ€è¦åœ°å€è¾“å…¥")
                    
            except Exception as e:
                logging.warning(f"åœ°å€å¤„ç†æ—¶å‡ºé”™: {str(e)}")
            
            # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°
            try:
                # ç­‰å¾…åº—é“ºåç§°æˆ–èœå•å®¹å™¨å‡ºç°
                await page.wait_for_selector('[data-testid="merchant-header"], .merchant-header, h1', timeout=30000)
            except TimeoutError:
                logging.warning("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæœªæ‰¾åˆ°å…³é”®å…ƒç´ ï¼Œç»§ç»­å°è¯•è§£æ...")
            
            # æå–åº—é“ºä¿¡æ¯
            shop_info = await _extract_shop_info_from_dom(page)
            logging.info(f"DOMè§£æåº—é“ºä¿¡æ¯ç»“æœ: {shop_info}")
            
            # æå–èœå•ä¿¡æ¯
            menu_info = await _extract_menu_info_from_dom(page)
            logging.info(f"DOMè§£æèœå•ä¿¡æ¯ç»“æœ: {menu_info}")
            
            return {
                "shop_info": shop_info,
                "menu": menu_info
            }
            
        except Exception as e:
            logging.error(f"DOMå¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {type(e).__name__} - {str(e)}")
            return {
                "shop_info": {"error": "DOMFallbackError", "message": f"DOMè§£æå¤±è´¥: {str(e)}"},
                "menu": {"error": "DOMFallbackError", "message": f"DOMè§£æå¤±è´¥: {str(e)}"}
            }
        finally:
            if browser:
                try:
                    if browser.is_connected():
                        contexts = browser.contexts
                        for context in contexts:
                            await context.close()
                        await browser.close()
                        logging.info("DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæµè§ˆå™¨å·²å…³é—­")
                except Exception as cleanup_error:
                    logging.warning(f"DOMå¤‡ç”¨æ–¹æ¡ˆï¼šæ¸…ç†æµè§ˆå™¨æ—¶å‡ºé”™: {cleanup_error}")

async def _extract_shop_info_from_dom(page) -> Dict[str, Any]:
    """ä»DOMä¸­æå–åº—é“ºä¿¡æ¯"""
    try:
        shop_info = {}
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        await page.wait_for_timeout(3000)  # ç­‰å¾…3ç§’è®©é¡µé¢å®Œå…¨åŠ è½½
        
        # æå–åº—é“ºåç§° - ä½¿ç”¨æ›´å¹¿æ³›çš„é€‰æ‹©å™¨
        name_selectors = [
            'h1',
            '[data-testid="merchant-header"] h1',
            '.merchant-header h1',
            '[data-testid="merchant-name"]',
            '.merchant-name',
            '.restaurant-name',
            '.store-name',
            'header h1',
            '.merchant-info h1',
            '.merchant-title',
            'div[class*="merchant"] h1',
            'div[class*="restaurant"] h1'
        ]
        
        for selector in name_selectors:
            try:
                name_element = await page.query_selector(selector)
                if name_element:
                    name_text = await name_element.text_content()
                    if name_text and name_text.strip():
                        shop_info['name'] = name_text.strip()
                        logging.info(f"æ‰¾åˆ°åº—é“ºåç§°: {shop_info['name']} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                        break
            except Exception:
                continue
        
        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°åç§°ï¼Œå°è¯•ä»é¡µé¢æ ‡é¢˜è·å–
        if not shop_info.get('name'):
            try:
                title = await page.title()
                if title and 'ifood' in title.lower():
                    # ä»æ ‡é¢˜ä¸­æå–åº—é“ºåç§°
                    import re
                    title_match = re.search(r'^([^-|]+)', title)
                    if title_match:
                        shop_info['name'] = title_match.group(1).strip()
                        logging.info(f"ä»é¡µé¢æ ‡é¢˜æå–åº—é“ºåç§°: {shop_info['name']}")
            except Exception:
                pass
        
        # æå–è¯„åˆ† - ä½¿ç”¨æ›´å¹¿æ³›çš„é€‰æ‹©å™¨
        rating_selectors = [
            '[data-testid="rating"]',
            '.rating',
            '.star-rating',
            '.merchant-rating',
            '.restaurant-rating',
            'div[class*="rating"]',
            'span[class*="rating"]',
            'div[class*="star"]'
        ]
        
        for selector in rating_selectors:
            try:
                rating_element = await page.query_selector(selector)
                if rating_element:
                    rating_text = await rating_element.text_content()
                    if rating_text:
                        import re
                        rating_match = re.search(r'(\d+[.,]\d+|\d+)', rating_text)
                        if rating_match:
                            rating_str = rating_match.group(1).replace(',', '.')
                            shop_info['rating'] = float(rating_str)
                            logging.info(f"æ‰¾åˆ°è¯„åˆ†: {shop_info['rating']} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                            break
            except Exception:
                continue
        
        # æå–é…é€æ—¶é—´
        delivery_selectors = [
            '[data-testid="delivery-time"]',
            '.delivery-time',
            '.delivery-info',
            'div[class*="delivery"]',
            'span[class*="delivery"]',
            'div[class*="time"]'
        ]
        
        for selector in delivery_selectors:
            try:
                delivery_element = await page.query_selector(selector)
                if delivery_element:
                    delivery_text = await delivery_element.text_content()
                    if delivery_text and delivery_text.strip():
                        shop_info['delivery_time'] = delivery_text.strip()
                        logging.info(f"æ‰¾åˆ°é…é€æ—¶é—´: {shop_info['delivery_time']} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                        break
            except Exception:
                continue
        
        # æå–é…é€è´¹
        fee_selectors = [
            '[data-testid="delivery-fee"]',
            '.delivery-fee',
            '.delivery-cost',
            'div[class*="fee"]',
            'span[class*="fee"]',
            'div[class*="cost"]'
        ]
        
        for selector in fee_selectors:
            try:
                fee_element = await page.query_selector(selector)
                if fee_element:
                    fee_text = await fee_element.text_content()
                    if fee_text and fee_text.strip():
                        shop_info['delivery_fee'] = fee_text.strip()
                        logging.info(f"æ‰¾åˆ°é…é€è´¹: {shop_info['delivery_fee']} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                        break
            except Exception:
                continue
        
        # æå–åœ°å€
        address_selectors = [
            '[data-testid="address"]',
            '.address',
            '.merchant-address',
            '.restaurant-address',
            'div[class*="address"]',
            'span[class*="address"]'
        ]
        
        for selector in address_selectors:
            try:
                address_element = await page.query_selector(selector)
                if address_element:
                    address_text = await address_element.text_content()
                    if address_text and address_text.strip():
                        shop_info['address'] = address_text.strip()
                        logging.info(f"æ‰¾åˆ°åœ°å€: {shop_info['address']} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                        break
            except Exception:
                continue
        
        # è®°å½•æ‰¾åˆ°çš„æ‰€æœ‰ä¿¡æ¯
        logging.info(f"DOMè§£ææå–åˆ°çš„åº—é“ºä¿¡æ¯: {shop_info}")
        
        if shop_info:
            return shop_info
        else:
            return {"error": "NoShopInfo", "message": "æœªèƒ½ä»DOMä¸­æå–åˆ°åº—é“ºä¿¡æ¯"}
            
    except Exception as e:
        logging.error(f"æå–åº—é“ºä¿¡æ¯å¤±è´¥: {str(e)}")
        return {"error": "ShopInfoExtractionError", "message": f"æå–åº—é“ºä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"}

async def _extract_menu_info_from_dom(page) -> Dict[str, Any]:
    """ä»DOMä¸­æå–èœå•ä¿¡æ¯"""
    try:
        menu_info = {"categories": []}
        
        # ç­‰å¾…èœå•åŠ è½½
        await page.wait_for_timeout(2000)  # ç­‰å¾…2ç§’è®©èœå•åŠ è½½
        
        # è°ƒè¯•ï¼šè®°å½•é¡µé¢åŸºæœ¬ä¿¡æ¯
        try:
            page_title = await page.title()
            logging.info(f"èœå•æå–ï¼šé¡µé¢æ ‡é¢˜: {page_title}")
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«åœ°å€ç›¸å…³æç¤º
            page_text = await page.text_content('body')
            if page_text:
                if "Escolha um endereÃ§o" in page_text or "Informe seu endereÃ§o" in page_text:
                    logging.warning("èœå•æå–ï¼šé¡µé¢ä»æ˜¾ç¤ºåœ°å€è¾“å…¥æç¤ºï¼Œå¯èƒ½åœ°å€è¾“å…¥æœªæˆåŠŸ")
                if "Buscando por" in page_text:
                    logging.info("èœå•æå–ï¼šé¡µé¢æ­£åœ¨æœç´¢ä¸­...")
                if "menu" in page_text.lower() or "cardÃ¡pio" in page_text.lower():
                    logging.info("èœå•æå–ï¼šé¡µé¢åŒ…å«èœå•ç›¸å…³å†…å®¹")
        except Exception as e:
            logging.warning(f"èœå•æå–ï¼šè·å–é¡µé¢åŸºæœ¬ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        # æŸ¥æ‰¾èœå•åˆ†ç±» - ä½¿ç”¨æ›´å¹¿æ³›çš„é€‰æ‹©å™¨
        category_selectors = [
            '[data-testid="category"]',
            '.category',
            '.menu-category',
            '.product-category',
            'section[class*="category"]',
            'div[class*="category"]',
            'section[class*="menu"]',
            'div[class*="menu-section"]',
            'div[class*="product-section"]',
            '.menu-group',
            '.product-group',
            '[data-testid="menu-section"]',
            '[data-testid="product-section"]',
            '.restaurant-menu',
            '.store-menu',
            '.merchant-menu'
        ]
        
        category_elements = []
        for selector in category_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    category_elements = elements
                    logging.info(f"æ‰¾åˆ° {len(elements)} ä¸ªåˆ†ç±»å…ƒç´  (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                    break
            except Exception:
                continue
        
        # å¦‚æœæ²¡æ‰¾åˆ°åˆ†ç±»ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å•†å“
        if not category_elements:
            logging.info("æœªæ‰¾åˆ°åˆ†ç±»ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾å•†å“...")
            item_selectors = [
                '[data-testid="menu-item"]',
                '[data-testid="product-item"]',
                '[data-testid="dish-item"]',
                '.menu-item',
                '.product-item',
                '.dish-item',
                '.food-item',
                'div[class*="product"]',
                'div[class*="item"]',
                'div[class*="dish"]',
                'article[class*="product"]',
                'article[class*="item"]',
                'article[class*="dish"]',
                'li[class*="product"]',
                'li[class*="item"]',
                'li[class*="dish"]'
            ]
            
            all_items = []
            for selector in item_selectors:
                try:
                    items = await page.query_selector_all(selector)
                    if items:
                        all_items = items
                        logging.info(f"æ‰¾åˆ° {len(items)} ä¸ªå•†å“ (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                        break
                except Exception:
                    continue
            
            if all_items:
                # åˆ›å»ºä¸€ä¸ªé€šç”¨åˆ†ç±»
                category_info = {
                    'name': 'èœå•',
                    'items': []
                }
                
                for item_element in all_items:
                    item_info = await _extract_item_info(item_element)
                    if item_info.get('name'):
                        category_info['items'].append(item_info)
                
                if category_info['items']:
                    menu_info['categories'].append(category_info)
        else:
            # å¤„ç†æ‰¾åˆ°çš„åˆ†ç±»
            for i, category_element in enumerate(category_elements):
                try:
                    category_info = {}
                    
                    # è°ƒè¯•ï¼šè®°å½•åˆ†ç±»å…ƒç´ çš„åŸºæœ¬ä¿¡æ¯
                    try:
                        element_html = await category_element.inner_html()
                        logging.info(f"å¤„ç†åˆ†ç±»å…ƒç´  {i+1}: HTMLé•¿åº¦={len(element_html)}")
                    except Exception:
                        pass
                    
                    # æå–åˆ†ç±»åç§° - ä½¿ç”¨æ›´å¹¿æ³›çš„é€‰æ‹©å™¨
                    name_selectors = [
                        'h2', 'h3', 'h4',
                        '.category-name',
                        '.category-title',
                        '.section-title',
                        '.menu-title',
                        'div[class*="title"]',
                        'span[class*="title"]',
                        'div[class*="name"]',
                        'span[class*="name"]'
                    ]
                    
                    for selector in name_selectors:
                        try:
                            name_element = await category_element.query_selector(selector)
                            if name_element:
                                name_text = await name_element.text_content()
                                if name_text and name_text.strip():
                                    category_info['name'] = name_text.strip()
                                    break
                        except Exception:
                            continue
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°åˆ†ç±»åç§°ï¼Œä½¿ç”¨é»˜è®¤åç§°
                    if not category_info.get('name'):
                        category_info['name'] = f'åˆ†ç±» {len(menu_info["categories"]) + 1}'
                    
                    # æå–è¯¥åˆ†ç±»ä¸‹çš„å•†å“
                    item_selectors = [
                        '[data-testid="menu-item"]',
                        '[data-testid="product-item"]',
                        '[data-testid="dish-item"]',
                        '.menu-item',
                        '.product-item',
                        '.dish-item',
                        '.food-item',
                        'div[class*="product"]',
                        'div[class*="item"]',
                        'div[class*="dish"]',
                        'article[class*="product"]',
                        'article[class*="item"]',
                        'article[class*="dish"]',
                        'li[class*="product"]',
                        'li[class*="item"]',
                        'li[class*="dish"]'
                    ]
                    
                    category_info['items'] = []
                    for selector in item_selectors:
                        try:
                            item_elements = await category_element.query_selector_all(selector)
                            if item_elements:
                                for item_element in item_elements:
                                    item_info = await _extract_item_info(item_element)
                                    if item_info.get('name'):
                                        category_info['items'].append(item_info)
                                break
                        except Exception:
                            continue
                    
                    # å³ä½¿æ²¡æœ‰å•†å“ï¼Œä¹Ÿä¿å­˜åˆ†ç±»ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    if category_info.get('name'):
                        if not category_info.get('items'):
                            category_info['items'] = []
                        menu_info['categories'].append(category_info)
                        logging.info(f"æå–åˆ†ç±»: {category_info['name']} ({len(category_info['items'])} ä¸ªå•†å“)")
                    else:
                        logging.warning(f"åˆ†ç±»å…ƒç´ æœªæ‰¾åˆ°åç§°ï¼Œè·³è¿‡")
                        
                except Exception as e:
                    logging.warning(f"æå–èœå•åˆ†ç±»æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        # è®°å½•ç»“æœ
        logging.info(f"DOMè§£ææå–åˆ°çš„èœå•ä¿¡æ¯: {len(menu_info['categories'])} ä¸ªåˆ†ç±»")
        
        # è¯¦ç»†è®°å½•æ¯ä¸ªåˆ†ç±»çš„å†…å®¹
        for i, category in enumerate(menu_info['categories']):
            category_name = category.get('name', 'æœªçŸ¥åˆ†ç±»')
            items_count = len(category.get('items', []))
            logging.info(f"  åˆ†ç±» {i+1}: {category_name} - å•†å“æ•°é‡: {items_count}")
            
            # è®°å½•å‰å‡ ä¸ªå•†å“çš„ä¿¡æ¯
            items = category.get('items', [])
            for j, item in enumerate(items[:3]):  # åªè®°å½•å‰3ä¸ªå•†å“
                if isinstance(item, dict):
                    item_name = item.get('name', 'æœªçŸ¥å•†å“')
                    item_price = item.get('price', 'æ— ä»·æ ¼')
                    logging.info(f"    å•†å“ {j+1}: {item_name} - ä»·æ ¼: {item_price}")
            
            if len(items) > 3:
                logging.info(f"    ... è¿˜æœ‰ {len(items) - 3} ä¸ªå•†å“")
        
        if menu_info['categories']:
            return menu_info
        else:
            return {"error": "NoMenuInfo", "message": "æœªèƒ½ä»DOMä¸­æå–åˆ°èœå•ä¿¡æ¯"}
            
    except Exception as e:
        logging.error(f"æå–èœå•ä¿¡æ¯å¤±è´¥: {str(e)}")
        return {"error": "MenuExtractionError", "message": f"æå–èœå•ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"}

async def _extract_item_info(item_element) -> Dict[str, Any]:
    """ä»å•ä¸ªå•†å“å…ƒç´ ä¸­æå–ä¿¡æ¯"""
    try:
        item_info = {}
        
        # å•†å“åç§° - ä½¿ç”¨æ›´å¹¿æ³›çš„é€‰æ‹©å™¨
        name_selectors = [
            '.item-name',
            '.product-name',
            '.dish-name',
            'h4', 'h5', 'h6',
            'div[class*="name"]',
            'span[class*="name"]',
            'div[class*="title"]',
            'span[class*="title"]',
            '.name',
            '.title'
        ]
        
        for selector in name_selectors:
            try:
                name_element = await item_element.query_selector(selector)
                if name_element:
                    name_text = await name_element.text_content()
                    if name_text and name_text.strip():
                        item_info['name'] = name_text.strip()
                        break
            except Exception:
                continue
        
        # å•†å“ä»·æ ¼ - ä½¿ç”¨æ›´å¹¿æ³›çš„é€‰æ‹©å™¨
        price_selectors = [
            '.price',
            '.item-price',
            '.product-price',
            '.cost',
            'div[class*="price"]',
            'span[class*="price"]',
            'div[class*="cost"]',
            'span[class*="cost"]'
        ]
        
        for selector in price_selectors:
            try:
                price_element = await item_element.query_selector(selector)
                if price_element:
                    price_text = await price_element.text_content()
                    if price_text:
                        # æ¸…ç†ä»·æ ¼æ–‡æœ¬
                        import re
                        price_match = re.search(r'R\$\s*(\d+[,.]?\d*)', price_text)
                        if price_match:
                            item_info['price'] = price_match.group(0)
                            break
            except Exception:
                continue
        
        # å•†å“æè¿° - ä½¿ç”¨æ›´å¹¿æ³›çš„é€‰æ‹©å™¨
        desc_selectors = [
            '.description',
            '.item-description',
            '.product-description',
            '.dish-description',
            'div[class*="description"]',
            'span[class*="description"]',
            'p[class*="description"]',
            '.desc',
            'p'
        ]
        
        for selector in desc_selectors:
            try:
                desc_element = await item_element.query_selector(selector)
                if desc_element:
                    desc_text = await desc_element.text_content()
                    if desc_text and desc_text.strip() and desc_text.strip() != item_info.get('name', ''):
                        item_info['description'] = desc_text.strip()
                        break
            except Exception:
                continue
        
        return item_info
        
    except Exception as e:
        logging.warning(f"æå–å•†å“ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        return {}

async def get_catalog_from_url(target_url: str, proxy_config: Optional[Dict[str, str]]) -> Dict[str, Any]:
    """
    ä½¿ç”¨é‡æ„åçš„æŠ“å–é€»è¾‘è®¿é—®iFoodé¡µé¢ï¼Œå¹¶ä»…æ‹¦æˆªèœå•ç›®å½•APIçš„å“åº”ã€‚
    """
    api_patterns = {"menu": re.compile(r"merchants/.*/catalog")}
    return await _scrape_ifood_page(target_url, proxy_config, api_patterns)

async def get_shop_info_from_url(target_url: str, proxy_config: Optional[Dict[str, str]]) -> Dict[str, Any]:
    """
    ä½¿ç”¨é‡æ„åçš„æŠ“å–é€»è¾‘è®¿é—®iFoodé¡µé¢ï¼Œå¹¶ä»…æ‹¦æˆªå•†æˆ·ä¿¡æ¯APIçš„å“åº”ã€‚
    """
    api_patterns = {"shop_info": re.compile(r"merchant-info/graphql")}
    return await _scrape_ifood_page(target_url, proxy_config, api_patterns)

async def get_shop_all_from_url(target_url: str, proxy_config: Optional[Dict[str, str]]) -> Dict[str, Any]:
    """
    ç›´æ¥ä½¿ç”¨APIæ‹¦æˆªç­–ç•¥è·å–åº—é“ºä¿¡æ¯å’Œèœå•æ•°æ®ï¼Œè·³è¿‡DOMè§£æã€‚
    """
    logging.info("ä½¿ç”¨APIæ‹¦æˆªç­–ç•¥è·å–åº—é“ºå’Œèœå•ä¿¡æ¯...")
    
    # åŒæ—¶æ‹¦æˆªåº—é“ºä¿¡æ¯å’Œèœå•API
    api_patterns = {
        "shop_info": re.compile(r"merchant-info/graphql"),
        "menu": re.compile(r"merchants/.*/catalog")
    }
    
    try:
        # ä½¿ç”¨APIæ‹¦æˆªè·å–æ•°æ®
        api_result = await _scrape_ifood_page(target_url, proxy_config, api_patterns)
        
        # æ£€æŸ¥APIæ‹¦æˆªç»“æœ
        shop_info = api_result.get("shop_info", {})
        menu = api_result.get("menu", {})
        
        # è®°å½•ç»“æœçŠ¶æ€
        if isinstance(shop_info, dict) and "error" not in shop_info:
            logging.info("APIæ‹¦æˆªæˆåŠŸè·å–åº—é“ºä¿¡æ¯")
        else:
            logging.warning("APIæ‹¦æˆªæœªèƒ½è·å–åº—é“ºä¿¡æ¯")
            
        if isinstance(menu, dict) and "error" not in menu:
            logging.info("APIæ‹¦æˆªæˆåŠŸè·å–èœå•ä¿¡æ¯")
        else:
            logging.warning("APIæ‹¦æˆªæœªèƒ½è·å–èœå•ä¿¡æ¯")
        
        return {
            "shop_info": shop_info,
            "menu": menu
        }
        
    except Exception as e:
        logging.error(f"APIæ‹¦æˆªè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return {
            "shop_info": {"error": "APIInterceptionError", "message": f"APIæ‹¦æˆªå¤±è´¥: {str(e)}"},
            "menu": {"error": "APIInterceptionError", "message": f"APIæ‹¦æˆªå¤±è´¥: {str(e)}"}
        }

# --- API ç«¯ç‚¹ ---

@app.get("/health", summary="å¥åº·æ£€æŸ¥", status_code=200)
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œè¿”å›æœåŠ¡çŠ¶æ€ä¿¡æ¯ã€‚
    """
    import psutil
    import platform
    
    try:
        # è·å–ç³»ç»Ÿä¿¡æ¯
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        health_info = {
            "status": "healthy",
            "timestamp": asyncio.get_event_loop().time(),
            "version": "1.0.0",
            "environment": {
                "is_cloud_function": IS_CLOUD_FUNCTION,
                "gcp_region": GCP_REGION if IS_CLOUD_FUNCTION else None,
                "proxy_strategy": PROXY_ROTATION_STRATEGY,
                "platform": platform.platform(),
                "python_version": platform.python_version()
            },
            "system": {
                "memory_usage_percent": memory.percent,
                "memory_available_mb": memory.available / 1024 / 1024,
                "cpu_percent": cpu_percent,
                "disk_usage_percent": psutil.disk_usage('/').percent
            },
                    "proxy": {
            "disabled": DISABLE_PROXY,
            "cloud_proxy_configured": bool(CLOUD_PROXY_HOST) if not DISABLE_PROXY else False,
            "local_proxy_file_exists": os.path.exists(PROXY_FILE) if not DISABLE_PROXY else False
        }
        }
        
        return health_info
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": asyncio.get_event_loop().time()
        }

@app.get("/status", summary="è¯¦ç»†çŠ¶æ€ä¿¡æ¯", status_code=200)
async def status_info(token: str = Depends(verify_token)):
    """
    è·å–è¯¦ç»†çš„ç³»ç»ŸçŠ¶æ€ä¿¡æ¯ï¼Œéœ€è¦è®¤è¯ã€‚
    """
    import psutil
    import platform
    from datetime import datetime
    
    # è·å–ä»£ç†é…ç½®ä¿¡æ¯
    proxy_config = get_random_proxy_config()
    proxy_info = "æ— ä»£ç†" if proxy_config is None else f"ä»£ç†: {proxy_config.get('server', 'æœªçŸ¥')}"
    
    # è·å–æ™ºèƒ½ä»£ç†ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
    proxy_stats = proxy_manager.get_proxy_stats_summary()
    
    status_info = {
        "service": "iFood Menu API",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat(),
        "uptime": asyncio.get_event_loop().time(),
        "proxy_config": proxy_info,
        "proxy_manager_stats": proxy_stats if not DISABLE_PROXY else {"disabled": True, "message": "ä»£ç†åŠŸèƒ½å·²ç¦ç”¨"},
        "environment": {
            "is_cloud_function": IS_CLOUD_FUNCTION,
            "gcp_region": GCP_REGION if IS_CLOUD_FUNCTION else None,
            "use_gcp_ip_rotation": USE_GCP_NATURAL_IP_ROTATION,
            "proxy_rotation_strategy": PROXY_ROTATION_STRATEGY
        },
        "system": {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "disk_total_gb": psutil.disk_usage('/').total / 1024 / 1024 / 1024
        }
    }
    
    return status_info

@app.post("/test", summary="æµ‹è¯•ä»£ç†å’Œæµè§ˆå™¨åŠŸèƒ½", status_code=200)
async def test_endpoint(token: str = Depends(verify_token)):
    """
    æµ‹è¯•ä»£ç†å’Œæµè§ˆå™¨åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
    ä¼šå°è¯•è®¿é—®ä¸€ä¸ªç®€å•çš„æµ‹è¯•é¡µé¢å¹¶è¿”å›ç»“æœã€‚
    """
    test_url = "https://httpbin.org/ip"
    proxy_config = get_random_proxy_config()
    
    try:
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            launch_options = {
                "headless": True,
                "timeout": 30000,
                "args": _get_optimized_browser_args()
            }
            
            if proxy_config:
                launch_options["proxy"] = proxy_config
                logging.info(f"ä½¿ç”¨ä»£ç†æµ‹è¯•: {proxy_config.get('server', 'æœªçŸ¥')}")
            
            browser = await _launch_browser_with_fallback(p, launch_options)
            
            try:
                page = await browser.new_page()
                page.set_default_navigation_timeout(30000)
                
                # è®¿é—®æµ‹è¯•é¡µé¢
                response = await page.goto(test_url, wait_until='domcontentloaded')
                
                if response and response.ok:
                    # è·å–é¡µé¢å†…å®¹
                    content = await page.text_content('body')
                    
                    # å°è¯•è§£æJSON
                    try:
                        import json
                        ip_info = json.loads(content)
                        test_result = {
                            "status": "success",
                            "message": "ä»£ç†å’Œæµè§ˆå™¨åŠŸèƒ½æ­£å¸¸",
                            "test_url": test_url,
                            "response_status": response.status,
                            "ip_info": ip_info,
                            "proxy_used": proxy_config is not None,
                            "proxy_server": proxy_config.get('server') if proxy_config else None
                        }
                    except json.JSONDecodeError:
                        test_result = {
                            "status": "partial_success",
                            "message": "é¡µé¢è®¿é—®æˆåŠŸä½†å†…å®¹è§£æå¤±è´¥",
                            "test_url": test_url,
                            "response_status": response.status,
                            "raw_content": content[:500] + "..." if len(content) > 500 else content,
                            "proxy_used": proxy_config is not None
                        }
                else:
                    test_result = {
                        "status": "failed",
                        "message": f"é¡µé¢è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status if response else 'unknown'}",
                        "test_url": test_url,
                        "proxy_used": proxy_config is not None
                    }
                    
            finally:
                await browser.close()
                
    except Exception as e:
        test_result = {
            "status": "error",
            "message": f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
            "error_type": type(e).__name__,
            "test_url": test_url,
            "proxy_used": proxy_config is not None
        }
    
    return test_result

@app.get("/debug/files", summary="è·å–è°ƒè¯•æ–‡ä»¶åˆ—è¡¨", status_code=200)
async def get_debug_files(token: str = Depends(verify_token)):
    """
    è·å–å¯ç”¨çš„è°ƒè¯•æ–‡ä»¶åˆ—è¡¨
    """
    import os
    import glob
    
    debug_files = []
    debug_dir = "/tmp/"
    
    # æŸ¥æ‰¾è°ƒè¯•æ–‡ä»¶
    patterns = [
        "page_*.png",
        "page_*.html",
        "*.png",
        "*.html"
    ]
    
    for pattern in patterns:
        files = glob.glob(os.path.join(debug_dir, pattern))
        for file_path in files:
            if os.path.exists(file_path):
                file_info = {
                    "name": os.path.basename(file_path),
                    "path": file_path,
                    "size": os.path.getsize(file_path),
                    "modified": os.path.getmtime(file_path)
                }
                debug_files.append(file_info)
    
    return {
        "debug_files": debug_files,
        "total_files": len(debug_files),
        "debug_directory": debug_dir
    }

@app.get("/debug/download/{filename}", summary="ä¸‹è½½è°ƒè¯•æ–‡ä»¶")
async def download_debug_file(filename: str, token: str = Depends(verify_token)):
    """
    ä¸‹è½½æŒ‡å®šçš„è°ƒè¯•æ–‡ä»¶
    """
    import os
    from fastapi.responses import FileResponse
    
    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ä¸‹è½½ç‰¹å®šç±»å‹çš„æ–‡ä»¶
    allowed_extensions = ['.png', '.html', '.txt', '.log']
    file_ext = os.path.splitext(filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(status_code=400, detail="ä¸å…è®¸ä¸‹è½½æ­¤ç±»å‹çš„æ–‡ä»¶")
    
    file_path = os.path.join("/tmp/", filename)
    
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type='application/octet-stream'
    )

@app.post("/get_menu", summary="è·å–iFoodåº—é“ºèœå•", status_code=200)
async def get_menu_endpoint(request: StoreRequest, token: str = Depends(verify_token)):
    """
    æ¥æ”¶ä¸€ä¸ªiFoodåº—é“ºçš„web URLï¼Œè¿”å›å…¶èœå•çš„JSONæ•°æ®ã€‚

    - **url**: iFoodåº—é“ºçš„å®Œæ•´URLã€‚
    - **éœ€è¦è®¤è¯**: è¯·æ±‚å¤´ä¸­å¿…é¡»åŒ…å« 'Authorization: Bearer your-super-secret-token'ã€‚
    """
    # æ·»åŠ äººç±»è¡Œä¸ºæ¨¡æ‹Ÿå»¶è¿Ÿ
    delay = get_human_like_delay()
    logging.info(f"æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œç­‰å¾… {delay:.2f} ç§’...")
    await asyncio.sleep(delay)
    
    proxy_config = get_random_proxy_config()
    
    # æ¸…ç†URLï¼Œå»æ‰æŸ¥è¯¢å‚æ•°
    cleaned_url = clean_url(request.url)

    logging.info(f"å¼€å§‹ä¸ºURLå¤„ç†è¯·æ±‚: {cleaned_url}")
    result = await get_catalog_from_url(cleaned_url, proxy_config)

    if "error" in result.get("menu", {}):
        status_code = result["menu"].get("status", 500)
        raise HTTPException(status_code=status_code, detail=result)
    
    return result

@app.post("/get_shop_info", summary="è·å–iFoodåº—é“ºè¯¦æƒ…", status_code=200)
async def get_shop_info_endpoint(request: StoreRequest, token: str = Depends(verify_token)):
    """
    æ¥æ”¶ä¸€ä¸ªiFoodåº—é“ºçš„web URLï¼Œä»…è¿”å›å…¶åº—é“ºçš„è¯¦ç»†ä¿¡æ¯ã€‚

    - **url**: iFoodåº—é“ºçš„å®Œæ•´URLã€‚
    - **éœ€è¦è®¤è¯**: è¯·æ±‚å¤´ä¸­å¿…é¡»åŒ…å« 'Authorization: Bearer your-super-secret-token'ã€‚
    """
    # æ·»åŠ äººç±»è¡Œä¸ºæ¨¡æ‹Ÿå»¶è¿Ÿ
    delay = get_human_like_delay()
    logging.info(f"æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œç­‰å¾… {delay:.2f} ç§’...")
    await asyncio.sleep(delay)
    
    proxy_config = get_random_proxy_config()
    
    # æ¸…ç†URLï¼Œå»æ‰æŸ¥è¯¢å‚æ•°
    cleaned_url = clean_url(request.url)

    logging.info(f"å¼€å§‹ä¸ºURLå¤„ç†è¯·æ±‚ï¼ˆä»…åº—é“ºä¿¡æ¯ï¼‰: {cleaned_url}")
    result = await get_shop_info_from_url(cleaned_url, proxy_config)

    if "error" in result.get("shop_info", {}):
        status_code = result["shop_info"].get("status", 500)
        raise HTTPException(status_code=status_code, detail=result)
    
    return result

@app.post("/get_shop_all", summary="è·å–iFoodåº—é“ºå…¨éƒ¨ä¿¡æ¯ï¼ˆèœå•+åº—é“ºè¯¦æƒ…ï¼‰", status_code=200)
async def get_shop_all_endpoint(request: StoreRequest, token: str = Depends(verify_token)):
    """
    æ¥æ”¶ä¸€ä¸ªiFoodåº—é“ºçš„web URLï¼Œè¿”å›å…¶èœå•å’Œåº—é“ºä¿¡æ¯çš„JSONæ•°æ®ã€‚

    - **url**: iFoodåº—é“ºçš„å®Œæ•´URLã€‚
    - **éœ€è¦è®¤è¯**: è¯·æ±‚å¤´ä¸­å¿…é¡»åŒ…å« 'Authorization: Bearer your-super-secret-token'ã€‚
    """
    # æ·»åŠ äººç±»è¡Œä¸ºæ¨¡æ‹Ÿå»¶è¿Ÿ
    delay = get_human_like_delay()
    logging.info(f"æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œç­‰å¾… {delay:.2f} ç§’...")
    await asyncio.sleep(delay)
    
    proxy_config = get_random_proxy_config()
    
    # æ¸…ç†URLï¼Œå»æ‰æŸ¥è¯¢å‚æ•°
    cleaned_url = clean_url(request.url)

    logging.info(f"å¼€å§‹ä¸ºURLå¤„ç†è¯·æ±‚ï¼ˆå…¨ä¿¡æ¯ï¼‰: {cleaned_url}")
    result = await get_shop_all_from_url(cleaned_url, proxy_config)

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å­ä»»åŠ¡éƒ½å¤±è´¥äº†
    all_failed = all("error" in v for v in result.values())
    if all_failed and result:
        any_error = next(iter(result.values()))
        status_code = any_error.get("status", 500)
        raise HTTPException(status_code=status_code, detail=result)

    return result

# --- å¦‚ä½•è¿è¡Œ ---
# 1. ç¡®ä¿ 'proxies.txt' æ–‡ä»¶å­˜åœ¨ä¸”åŒ…å«æœ‰æ•ˆçš„SOCKS5ä»£ç†ã€‚
# 2. å®‰è£…ä¾èµ–: pip install -r requirements.txt
# 3. è¿è¡ŒPlaywrightçš„æµè§ˆå™¨å®‰è£…: playwright install
# 4. å¯åŠ¨æœåŠ¡å™¨: uvicorn api:app --reload
# 5. åœ¨ http://127.0.0.1:8000/docs æŸ¥çœ‹APIæ–‡æ¡£å¹¶æµ‹è¯•ã€‚

# --- ä¸»ç¨‹åºå…¥å£ç‚¹ ---
if __name__ == "__main__":
    import uvicorn
    
    # é…ç½®æœåŠ¡å™¨å‚æ•°
    host = "0.0.0.0"
    port = 8000
    reload = True
    
    print(f"ğŸš€ å¯åŠ¨ iFood Menu API æœåŠ¡å™¨...")
    print(f"ğŸ“ æœåŠ¡å™¨åœ°å€: http://{host}:{port}")
    print(f"ğŸ“š API æ–‡æ¡£: http://{host}:{port}/docs")
    print(f"ğŸ”‘ è®¤è¯ä»¤ç‰Œ: {API_TOKEN}")
    print(f"ğŸŒ ä»£ç†ç­–ç•¥: {PROXY_ROTATION_STRATEGY}")
    print(f"â˜ï¸  Cloud Function æ¨¡å¼: {IS_CLOUD_FUNCTION}")
    
    if IS_CLOUD_FUNCTION:
        print(f"ğŸŒ GCP åŒºåŸŸ: {GCP_REGION}")
        print(f"ğŸ”„ GCP IP è½®æ¢: {USE_GCP_NATURAL_IP_ROTATION}")
    
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        "api:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info"
    )
